"""
SVRaster ä½¿ç”¨ç¤ºä¾‹å’ŒæŒ‡å—

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€å®ç”¨çš„ SVRaster ä½¿ç”¨ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•è¿›è¡Œæ¨ç†å’Œæ¸²æŸ“ã€‚
"""

import torch
import numpy as np
import time
from typing import Dict, Tuple, Optional

def create_test_scene():
    """åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•åœºæ™¯"""
    
    # åœºæ™¯å‚æ•°
    resolution = 64
    scene_bounds = (-1.0, -1.0, -1.0, 1.0, 1.0, 1.0)
    
    # åˆ›å»ºåæ ‡ç½‘æ ¼
    coords = torch.stack(torch.meshgrid(
        torch.linspace(-1, 1, resolution),
        torch.linspace(-1, 1, resolution),
        torch.linspace(-1, 1, resolution),
        indexing='ij'
    ), dim=-1)
    
    # åˆ›å»ºä¸¤ä¸ªçƒä½“
    center1 = torch.tensor([0.3, 0.0, 0.0])
    center2 = torch.tensor([-0.3, 0.0, 0.0])
    
    dist1 = torch.norm(coords - center1, dim=-1)
    dist2 = torch.norm(coords - center2, dim=-1)
    
    # å¯†åº¦åˆ†å¸ƒ
    density1 = torch.exp(-dist1 * 4.0) * 0.8
    density2 = torch.exp(-dist2 * 3.0) * 0.6
    densities = density1 + density2
    
    # é¢œè‰²ç‰¹å¾
    colors = torch.zeros(*coords.shape)  # ä¸coordsç›¸åŒçš„å½¢çŠ¶
    mask1 = dist1 < 0.3
    mask2 = dist2 < 0.3
    colors[mask1, 0] = 1.0  # çº¢è‰²çƒ
    colors[mask2, 2] = 1.0  # è“è‰²çƒ
    
    return {
        'positions': coords.reshape(-1, 3),
        'densities': densities.reshape(-1),
        'colors': colors.reshape(-1, 3),
        'sizes': torch.full((resolution**3,), 0.1),
        'scene_bounds': scene_bounds
    }


def generate_camera_rays(camera_pos: torch.Tensor, 
                        target_pos: torch.Tensor,
                        image_size: Tuple[int, int],
                        num_rays: int = 1000) -> Tuple[torch.Tensor, torch.Tensor]:
    """ç”Ÿæˆç›¸æœºå…‰çº¿"""
    
    # ç›¸æœºæ–¹å‘
    forward = target_pos - camera_pos
    forward = forward / torch.norm(forward)
    
    # æ„å»ºç›¸æœºåæ ‡ç³»
    up = torch.tensor([0.0, 1.0, 0.0])
    right = torch.cross(forward, up)
    right = right / torch.norm(right)
    up = torch.cross(right, forward)
    
    # éšæœºé‡‡æ ·å›¾åƒåƒç´ 
    width, height = image_size
    pixel_coords = torch.rand(num_rays, 2) * 2 - 1  # [-1, 1]
    
    # è®¡ç®—å…‰çº¿æ–¹å‘
    ray_directions = (pixel_coords[:, 0:1] * right * 0.5 + 
                     pixel_coords[:, 1:2] * up * 0.5 + 
                     forward)
    ray_directions = ray_directions / torch.norm(ray_directions, dim=1, keepdim=True)
    
    # æ‰€æœ‰å…‰çº¿ä»ç›¸æœºä½ç½®å¼€å§‹
    ray_origins = camera_pos.unsqueeze(0).expand(num_rays, -1)
    
    return ray_origins, ray_directions


def volume_rendering(voxel_data: Dict, 
                    rays_o: torch.Tensor, 
                    rays_d: torch.Tensor,
                    t_near: float = 0.1,
                    t_far: float = 4.0,
                    num_samples: int = 64) -> Dict[str, torch.Tensor]:
    """ç®€åŒ–çš„ä½“ç§¯æ¸²æŸ“ï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰"""
    
    device = rays_o.device
    num_rays = rays_o.shape[0]
    
    # æ²¿å…‰çº¿é‡‡æ ·
    t_vals = torch.linspace(t_near, t_far, num_samples, device=device)
    t_vals = t_vals.unsqueeze(0).expand(num_rays, -1)
    
    # æ·»åŠ éšæœºæ‰°åŠ¨
    t_vals = t_vals + torch.rand_like(t_vals) * (t_far - t_near) / num_samples
    
    # è®¡ç®—é‡‡æ ·ç‚¹
    pts = rays_o.unsqueeze(1) + rays_d.unsqueeze(1) * t_vals.unsqueeze(2)
    
    # æŸ¥è¯¢ä½“ç´ å¯†åº¦å’Œé¢œè‰²ï¼ˆç®€åŒ–ç‰ˆï¼‰
    densities = torch.zeros(num_rays, num_samples, device=device)
    colors = torch.zeros(num_rays, num_samples, 3, device=device)
    
    # ç®€åŒ–çš„ä½“ç´ æŸ¥è¯¢
    for i in range(num_rays):
        for j in range(num_samples):
            pt = pts[i, j]
            # ç®€å•çš„è·ç¦»æŸ¥è¯¢
            dist = torch.norm(pt)
            if dist < 1.0:
                densities[i, j] = torch.exp(-dist * 2.0)
                colors[i, j] = torch.sigmoid(pt)  # ç®€å•çš„é¢œè‰²æ˜ å°„
    
    # ä½“ç§¯æ¸²æŸ“ç§¯åˆ†
    delta = t_vals[:, 1:] - t_vals[:, :-1]
    delta = torch.cat([delta, torch.full_like(delta[:, :1], 1e10)], dim=1)
    
    alpha = 1.0 - torch.exp(-densities * delta)
    transmittance = torch.cumprod(1.0 - alpha + 1e-10, dim=1)
    transmittance = torch.cat([torch.ones_like(transmittance[:, :1]), transmittance[:, :-1]], dim=1)
    
    weights = alpha * transmittance
    
    # æœ€ç»ˆé¢œè‰²å’Œæ·±åº¦
    rgb = torch.sum(weights.unsqueeze(2) * colors, dim=1)
    depth = torch.sum(weights * t_vals, dim=1)
    
    return {
        'rgb': rgb,
        'depth': depth,
        'weights': weights
    }


def rasterization_rendering(voxel_data: Dict,
                           rays_o: torch.Tensor,
                           rays_d: torch.Tensor) -> Dict[str, torch.Tensor]:
    """ç®€åŒ–çš„å…‰æ …åŒ–æ¸²æŸ“ï¼ˆæ¨ç†æ¨¡å¼ï¼‰"""
    
    device = rays_o.device
    num_rays = rays_o.shape[0]
    
    # ç®€åŒ–çš„å…‰æ …åŒ–ï¼šç›´æ¥æŸ¥è¯¢æœ€è¿‘çš„ä½“ç´ 
    rgb = torch.zeros(num_rays, 3, device=device)
    depth = torch.full((num_rays,), 4.0, device=device)
    
    # å¯¹æ¯æ¡å…‰çº¿è¿›è¡Œç®€åŒ–çš„å…‰æ …åŒ–
    for i in range(num_rays):
        ray_o = rays_o[i]
        ray_d = rays_d[i]
        
        # ç®€å•çš„å…‰çº¿æ­¥è¿›
        for t in torch.linspace(0.1, 4.0, 32, device=device):
            pt = ray_o + ray_d * t
            
            # æ£€æŸ¥æ˜¯å¦åœ¨åœºæ™¯å†…
            if torch.all(torch.abs(pt) < 1.0):
                # ç®€å•çš„å¯†åº¦æŸ¥è¯¢
                dist = torch.norm(pt)
                if dist < 0.5:
                    density = torch.exp(-dist * 3.0)
                    if density > 0.1:
                        rgb[i] = torch.sigmoid(pt)
                        depth[i] = t
                        break
    
    return {
        'rgb': rgb,
        'depth': depth
    }


def demo_svraster_usage():
    """æ¼”ç¤º SVRaster ä½¿ç”¨æ–¹æ³•"""
    
    print("=== SVRaster ä½¿ç”¨æ¼”ç¤º ===\n")
    
    # æ£€æŸ¥è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. åˆ›å»ºæµ‹è¯•åœºæ™¯
    print("\n1. åˆ›å»ºæµ‹è¯•åœºæ™¯...")
    scene_data = create_test_scene()
    print(f"   - ä½“ç´ æ•°é‡: {len(scene_data['positions'])}")
    print(f"   - å¯†åº¦èŒƒå›´: [{scene_data['densities'].min():.3f}, {scene_data['densities'].max():.3f}]")
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    for key in scene_data:
        if isinstance(scene_data[key], torch.Tensor):
            scene_data[key] = scene_data[key].to(device)
    
    # 2. è®¾ç½®ç›¸æœº
    print("\n2. è®¾ç½®ç›¸æœº...")
    camera_pos = torch.tensor([0.0, 0.0, 2.0], device=device)
    target_pos = torch.tensor([0.0, 0.0, 0.0], device=device)
    image_size = (400, 300)
    num_rays = 500
    
    # ç”Ÿæˆå…‰çº¿
    rays_o, rays_d = generate_camera_rays(camera_pos, target_pos, image_size, num_rays)
    rays_o = rays_o.to(device)
    rays_d = rays_d.to(device)
    
    print(f"   - ç›¸æœºä½ç½®: {camera_pos}")
    print(f"   - ç›®æ ‡ä½ç½®: {target_pos}")
    print(f"   - å…‰çº¿æ•°é‡: {num_rays}")
    
    # 3. è®­ç»ƒæ¨¡å¼æ¸²æŸ“ï¼ˆä½“ç§¯æ¸²æŸ“ï¼‰
    print("\n3. è®­ç»ƒæ¨¡å¼æ¸²æŸ“ï¼ˆä½“ç§¯æ¸²æŸ“ï¼‰...")
    start_time = time.time()
    training_outputs = volume_rendering(scene_data, rays_o, rays_d)
    training_time = time.time() - start_time
    
    print(f"   - æ¸²æŸ“æ—¶é—´: {training_time:.3f}ç§’")
    print(f"   - RGBå½¢çŠ¶: {training_outputs['rgb'].shape}")
    print(f"   - RGBèŒƒå›´: [{training_outputs['rgb'].min():.3f}, {training_outputs['rgb'].max():.3f}]")
    print(f"   - æ·±åº¦èŒƒå›´: [{training_outputs['depth'].min():.3f}, {training_outputs['depth'].max():.3f}]")
    
    # 4. æ¨ç†æ¨¡å¼æ¸²æŸ“ï¼ˆå…‰æ …åŒ–ï¼‰
    print("\n4. æ¨ç†æ¨¡å¼æ¸²æŸ“ï¼ˆå…‰æ …åŒ–ï¼‰...")
    start_time = time.time()
    inference_outputs = rasterization_rendering(scene_data, rays_o, rays_d)
    inference_time = time.time() - start_time
    
    print(f"   - æ¸²æŸ“æ—¶é—´: {inference_time:.3f}ç§’")
    print(f"   - RGBå½¢çŠ¶: {inference_outputs['rgb'].shape}")
    print(f"   - RGBèŒƒå›´: [{inference_outputs['rgb'].min():.3f}, {inference_outputs['rgb'].max():.3f}]")
    print(f"   - æ·±åº¦èŒƒå›´: [{inference_outputs['depth'].min():.3f}, {inference_outputs['depth'].max():.3f}]")
    
    # 5. æ€§èƒ½æ¯”è¾ƒ
    print("\n5. æ€§èƒ½æ¯”è¾ƒ...")
    if training_time > 0 and inference_time > 0:
        speedup = training_time / inference_time
        print(f"   - é€Ÿåº¦æå‡: {speedup:.2f}x")
    
    rays_per_sec_training = num_rays / training_time if training_time > 0 else 0
    rays_per_sec_inference = num_rays / inference_time if inference_time > 0 else 0
    
    print(f"   - è®­ç»ƒæ¨¡å¼: {rays_per_sec_training:.0f} å…‰çº¿/ç§’")
    print(f"   - æ¨ç†æ¨¡å¼: {rays_per_sec_inference:.0f} å…‰çº¿/ç§’")
    
    # 6. è´¨é‡åˆ†æ
    print("\n6. è´¨é‡åˆ†æ...")
    rgb_diff = torch.mean(torch.abs(training_outputs['rgb'] - inference_outputs['rgb']))
    depth_diff = torch.mean(torch.abs(training_outputs['depth'] - inference_outputs['depth']))
    
    print(f"   - RGBå·®å¼‚: {rgb_diff:.4f}")
    print(f"   - æ·±åº¦å·®å¼‚: {depth_diff:.4f}")
    
    return training_outputs, inference_outputs


def show_complete_usage_guide():
    """æ˜¾ç¤ºå®Œæ•´çš„ä½¿ç”¨æŒ‡å—"""
    
    print("\n" + "="*50)
    print("SVRaster å®Œæ•´ä½¿ç”¨æŒ‡å—")
    print("="*50)
    
    print("""
ğŸ¯ SVRaster æ˜¯ä»€ä¹ˆï¼Ÿ
SVRaster æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„ç¥ç»è¾å°„åœºæ¸²æŸ“ç³»ç»Ÿï¼Œç»“åˆäº†ä¸¤ç§æ¸²æŸ“æ–¹å¼ï¼š
- è®­ç»ƒæ¨¡å¼ï¼šä½“ç§¯æ¸²æŸ“ï¼Œå‡†ç¡®ä½†æ…¢
- æ¨ç†æ¨¡å¼ï¼šå…‰æ …åŒ–ï¼Œå¿«é€Ÿä½†è¿‘ä¼¼

ğŸ“‹ åŸºæœ¬ä½¿ç”¨æµç¨‹ï¼š
1. åˆ›å»ºåœºæ™¯é…ç½®
2. åˆå§‹åŒ–æ¨¡å‹
3. ç”Ÿæˆç›¸æœºå…‰çº¿
4. é€‰æ‹©æ¸²æŸ“æ¨¡å¼
5. è·å–æ¸²æŸ“ç»“æœ

ğŸ’¡ å®é™…ä½¿ç”¨ä»£ç ï¼š
```python
# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from src.nerfs.svraster import SVRasterModel, SVRasterConfig

# 1. åˆ›å»ºé…ç½®
config = SVRasterConfig(
    image_width=800,
    image_height=600,
    scene_bounds=(-2.0, -2.0, -2.0, 2.0, 2.0, 2.0),
    grid_resolution=128,
    sh_degree=2
)

# 2. åˆå§‹åŒ–æ¨¡å‹
model = SVRasterModel(config)

# 3. å‡†å¤‡å…‰çº¿æ•°æ®
# å…‰çº¿èµ·ç‚¹ï¼ˆç›¸æœºä½ç½®ï¼‰
rays_o = torch.tensor([[0.0, 0.0, 3.0]]).expand(1000, -1)

# å…‰çº¿æ–¹å‘ï¼ˆä»ç›¸æœºæŒ‡å‘åœºæ™¯ï¼‰
rays_d = torch.randn(1000, 3)
rays_d = rays_d / torch.norm(rays_d, dim=1, keepdim=True)

# 4. æ¨ç†æ¸²æŸ“ï¼ˆå¿«é€Ÿï¼‰
with torch.no_grad():
    outputs = model(rays_o, rays_d, mode="inference")

# 5. è®­ç»ƒæ¸²æŸ“ï¼ˆç²¾ç¡®ï¼‰
with torch.no_grad():
    outputs = model(rays_o, rays_d, mode="training")

# 6. è·å–ç»“æœ
rgb = outputs['rgb']      # é¢œè‰² [N, 3]
depth = outputs['depth']  # æ·±åº¦ [N]
```

ğŸ”§ å…³é”®å‚æ•°è¯´æ˜ï¼š
- image_width/height: å›¾åƒåˆ†è¾¨ç‡
- scene_bounds: åœºæ™¯è¾¹ç•Œ (x_min, y_min, z_min, x_max, y_max, z_max)
- grid_resolution: ä½“ç´ ç½‘æ ¼åˆ†è¾¨ç‡ï¼ˆå½±å“è´¨é‡å’Œé€Ÿåº¦ï¼‰
- sh_degree: çƒè°å‡½æ•°é˜¶æ•°ï¼ˆå½±å“å…‰ç…§è´¨é‡ï¼‰

ğŸ“Š ä¸¤ç§æ¸²æŸ“æ¨¡å¼ï¼š

è®­ç»ƒæ¨¡å¼ (mode="training"):
- ä½¿ç”¨ä½“ç§¯æ¸²æŸ“
- æ²¿å…‰çº¿ç§¯åˆ†
- æ›´å‡†ç¡®ï¼Œè´¨é‡æ›´é«˜
- é€Ÿåº¦è¾ƒæ…¢
- é€‚åˆè®­ç»ƒå’Œé«˜è´¨é‡æ¸²æŸ“

æ¨ç†æ¨¡å¼ (mode="inference"):
- ä½¿ç”¨å…‰æ …åŒ–
- ç›´æ¥æŠ•å½±ä½“ç´ 
- é€Ÿåº¦æ›´å¿«
- è´¨é‡ç•¥ä½
- é€‚åˆå®æ—¶æ¸²æŸ“å’Œæ¨ç†

âš¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼š
1. ä½¿ç”¨ torch.no_grad() è¿›è¡Œæ¨ç†
2. æ‰¹é‡å¤„ç†å…‰çº¿æé«˜æ•ˆç‡
3. è°ƒæ•´ grid_resolution å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦
4. æ¨ç†æ¨¡å¼ç”¨äºå®æ—¶åº”ç”¨
5. ä½¿ç”¨ GPU åŠ é€Ÿè®¡ç®—

ğŸ¨ åº”ç”¨åœºæ™¯ï¼š
- ç¥ç»è¾å°„åœºæ¸²æŸ“
- 3D åœºæ™¯é‡å»º
- è™šæ‹Ÿç°å®æ¸²æŸ“
- æ¸¸æˆå¼•æ“é›†æˆ
- å½±è§†ç‰¹æ•ˆåˆ¶ä½œ

ğŸ” è°ƒè¯•æŠ€å·§ï¼š
1. ä»å°çš„ grid_resolution å¼€å§‹æµ‹è¯•
2. æ£€æŸ¥å…‰çº¿æ•°æ®çš„åˆç†æ€§
3. éªŒè¯åœºæ™¯è¾¹ç•Œè®¾ç½®
4. æ¯”è¾ƒä¸¤ç§æ¨¡å¼çš„è¾“å‡ºå·®å¼‚
5. ç›‘æ§å†…å­˜å’Œè®¡ç®—æ—¶é—´

ğŸ“ˆ æ‰©å±•åŠŸèƒ½ï¼š
- æ”¯æŒä¸åŒçš„æŸå¤±å‡½æ•°
- å¯è‡ªå®šä¹‰é‡‡æ ·ç­–ç•¥
- æ”¯æŒå¤šç§æ¿€æ´»å‡½æ•°
- å¯é…ç½®çš„æ¸²æŸ“å‚æ•°
- çµæ´»çš„ç›¸æœºæ¨¡å‹
""")


if __name__ == "__main__":
    try:
        # è¿è¡Œæ¼”ç¤º
        print("ğŸš€ å¼€å§‹ SVRaster æ¼”ç¤º...")
        training_outputs, inference_outputs = demo_svraster_usage()
        
        # æ˜¾ç¤ºå®Œæ•´æŒ‡å—
        show_complete_usage_guide()
        
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("ç°åœ¨æ‚¨å¯ä»¥å¼€å§‹ä½¿ç”¨ SVRaster è¿›è¡Œç¥ç»è¾å°„åœºæ¸²æŸ“äº†ï¼")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ¼”ç¤º")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
