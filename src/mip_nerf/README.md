# Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields

This directory contains a complete implementation of Mip-NeRF, which addresses aliasing artifacts in Neural Radiance Fields by representing each pixel as a cone rather than a ray, and using integrated positional encoding.

## Overview

Mip-NeRF improves upon the original NeRF by:

1. **Integrated Positional Encoding (IPE)**: Instead of encoding individual points, IPE encodes entire conical frustums by integrating the positional encoding over the volume of the frustum.

2. **Conical Frustum Representation**: Each pixel is represented as a conical frustum in 3D space, accounting for the finite size of pixels and anti-aliasing.

3. **Multi-scale Rendering**: The method naturally handles varying levels of detail across different viewing distances.

4. **Anti-aliasing**: Reduces aliasing artifacts that are common in the original NeRF, especially when rendering at different resolutions.

## Key Features

- ✅ **Integrated Positional Encoding**: Handles frustum volumes instead of point samples
- ✅ **Conical Frustum Sampling**: Proper pixel footprint modeling
- ✅ **Hierarchical Sampling**: Coarse-to-fine sampling strategy
- ✅ **Multi-scale Loss**: Training with multiple resolution levels
- ✅ **Comprehensive Training Pipeline**: Full training, validation, and testing
- ✅ **Multiple Dataset Support**: Blender synthetic and LLFF real scenes
- ✅ **Visualization Tools**: Training curves, rendered images, and video generation

## Architecture

### Core Components

- **`MipNeRF`**: Main model class combining coarse and fine networks
- **`IntegratedPositionalEncoder`**: IPE implementation for encoding frustums
- **`ConicalFrustum`**: Representation of pixel cones in 3D space
- **`MipNeRFMLP`**: Neural network with integrated positional encoding
- **`MipNeRFRenderer`**: Volumetric rendering with anti-aliasing

### Training Components

- **`MipNeRFTrainer`**: Complete training pipeline with validation and testing
- **`MipNeRFLoss`**: Loss function with coarse and fine network components
- **Dataset Classes**: Support for Blender and LLFF datasets

## Installation

The implementation requires the following dependencies:

```bash
torch >= 1.9.0
torchvision
numpy
opencv-python
imageio
matplotlib
tqdm
tensorboard
PIL
pathlib
```

## Usage

### Basic Training

```python
from src.mip_nerf import MipNeRF, MipNeRFConfig, MipNeRFTrainer
from src.mip_nerf.dataset import create_mip_nerf_dataset

# Configuration
config = MipNeRFConfig(
    netdepth=8,
    netwidth=256,
    num_samples=64,
    num_importance=128,
    use_viewdirs=True,
    lr_init=5e-4,
    lr_final=5e-6
)

# Load datasets
train_dataset = create_mip_nerf_dataset(
    data_dir="path/to/dataset",
    dataset_type='blender',
    split='train'
)

val_dataset = create_mip_nerf_dataset(
    data_dir="path/to/dataset", 
    dataset_type='blender',
    split='val'
)

# Create and train model
model = MipNeRF(config)
trainer = MipNeRFTrainer(config, model, train_dataset, val_dataset)
trainer.train(num_epochs=100)
```

### Inference

```python
# Load trained model
model = MipNeRF(config)
checkpoint = torch.load('path/to/checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])

# Render rays
with torch.no_grad():
    results = model(origins, directions, viewdirs, near=2.0, far=6.0)
    rgb = results['fine']['rgb']  # or results['coarse']['rgb']
```

## Dataset Format

### Blender Synthetic Dataset

The implementation supports the standard NeRF Blender dataset format:

```Mega-NeRF++: An Improved Scalable NeRFs for High-resolution Photogrammetric Images
dataset/
├── transforms_train.json
├── transforms_val.json  
├── transforms_test.json
├── train/
│   ├── r_0.png
│   ├── r_1.png
│   └── ...
├── val/
└── test/
```

### LLFF Real ScenesMega-NeRF++: An Improved Scalable NeRFs for High-resolution Photogrammetric Images

Also supports LLFF format datasets:

```
dataset/
├── poses_bounds.npy
└── images/
    ├── IMG_0001.jpg
    ├── IMG_0002.jpg
    └── ...
```

## Configuration Options

Key configuration parameters in `MipNeRFConfig`:

- **`netdepth`**: Number of layers in the MLP (default: 8)
- **`netwidth`**: Number of neurons per layer (default: 256)
- **`num_samples`**: Number of coarse samples per ray (default: 64)
- **`num_importance`**: Number of fine samples per ray (default: 128)
- **`min_deg_point`**: Minimum degree for positional encoding (default: 0)
- **`max_deg_point`**: Maximum degree for positional encoding (default: 12)
- **`use_viewdirs`**: Whether to use view directions (default: True)
- **`lr_init`**: Initial learning rate (default: 5e-4)
- **`lr_final`**: Final learning rate (default: 5e-6)

## Training Tips

1. **Start with lower resolution** for faster experimentation
2. **Use white background** for transparent objects in Blender scenes
3. **Monitor PSNR** during training - should increase steadily
4. **Adjust learning rate schedule** based on convergence
5. **Use validation set** to prevent overfitting

## Differences from Original NeRF

| Feature | Original NeRF | Mip-NeRF |
|---------|---------------|----------|
| Positional Encoding | Point-wise PE | Integrated PE (IPE) |
| Ray Representation | Infinitesimal rays | Conical frustums |
| Anti-aliasing | None | Built-in through IPE |
| Multi-scale | Manual tricks | Natural handling |
| Pixel footprint | Ignored | Properly modeled |

## Mathematical Foundation

### Integrated Positional Encoding

For a multivariate Gaussian with mean μ and covariance Σ:

```
IPE(μ, Σ) = [E[sin(2^j μ)], E[cos(2^j μ)]] for j = 0, ..., L-1
```

Where:
```
E[sin(x)] = exp(-σ²/2) * sin(μ)
E[cos(x)] = exp(-σ²/2) * cos(μ)
```

### Conical Frustum to Gaussian

Each pixel cone is approximated as a 3D Gaussian with:
- **Mean**: Center of the frustum
- **Covariance**: Combines axial (along ray) and radial (perpendicular) variance

## Performance

Expected performance on standard datasets:

| Dataset | PSNR | Training Time | Memory |
|---------|------|---------------|--------|
| Lego (Blender) | ~32 dB | ~8 hours (RTX 3080) | ~8GB |
| Chair (Blender) | ~34 dB | ~8 hours (RTX 3080) | ~8GB |
| LLFF Scenes | ~26 dB | ~12 hours (RTX 3080) | ~10GB |

## Visualization

The implementation includes visualization utilities:

- **Training curves**: Loss and PSNR over time
- **Rendered images**: Comparison between prediction and ground truth
- **Depth maps**: Visualize learned geometry
- **Video generation**: Novel view synthesis videos

## Testing

Run the test suite:

```python
python -m src.mip_nerf.test_mip_nerf
```

## Examples

See `example_usage.py` for detailed examples of:
- Training on synthetic data
- Inference with trained models
- Novel view synthesis
- Debugging integrated positional encoding

## Citation

If you use this implementation, please cite the original Mip-NeRF paper:

```bibtex
@inproceedings{barron2021mipnerf,
  title={Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields},
  author={Barron, Jonathan T and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P},
  booktitle={International Conference on Computer Vision (ICCV)},
  year={2021}
}
```

## License

This implementation is provided for research and educational purposes.

## Troubleshooting

### Common Issues

1. **CUDA out of memory**: Reduce batch size or image resolution
2. **Slow training**: Enable mixed precision or reduce network size
3. **Poor quality**: Check dataset format and camera poses
4. **NaN losses**: Reduce learning rate or check data preprocessing

### Performance Optimization

- Use `torch.compile()` for PyTorch 2.0+
- Enable automatic mixed precision (AMP)
- Use multiple GPUs with DataParallel
- Optimize batch size based on available memory

### Debugging

Enable debug mode in training:

```python
trainer.train(num_epochs=100, log_freq=10)  # More frequent logging
```

Check integrated positional encoding:

```python
from src.mip_nerf.example_usage import debug_integrated_positional_encoding
debug_integrated_positional_encoding()
```

## Future Improvements

Potential enhancements to this implementation:

- [ ] Support for unbounded scenes (Mip-NeRF 360)
- [ ] Distortion loss for better geometry
- [ ] Proposal networks for faster sampling
- [ ] Support for HDR images
- [ ] Multi-GPU training
- [ ] TensorRT optimization for inference
- [ ] Web-based viewer integration 