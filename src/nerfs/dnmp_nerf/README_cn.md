# DNMP-NeRF: åŸºäºå¯å˜å½¢ç¥ç»ç½‘æ ¼åŸºå…ƒçš„åŸå¸‚è¾å°„åœº

æœ¬æ¨¡å—å®ç°äº†è®ºæ–‡ "Urban Radiance Field Representation with Deformable Neural Mesh Primitives" (ICCV 2023) ä¸­æè¿°çš„ DNMP (å¯å˜å½¢ç¥ç»ç½‘æ ¼åŸºå…ƒ) æ–¹æ³•ï¼Œç”¨äºé«˜æ•ˆçš„åŸå¸‚è§„æ¨¡ç¥ç»è¾å°„åœºæ¸²æŸ“ã€‚

## æ¦‚è¿°

DNMP-NeRF ä½¿ç”¨ä¸€ç»„å¯å˜å½¢ç¥ç»ç½‘æ ¼åŸºå…ƒæ¥è¡¨ç¤ºå¤§è§„æ¨¡åŸå¸‚åœºæ™¯ã€‚æ¯ä¸ªåŸºå…ƒç”±æ§åˆ¶ç½‘æ ¼å‡ ä½•å½¢çŠ¶çš„å¯å­¦ä¹ æ½œåœ¨ç¼–ç å’Œå†…åµŒè¾å°„ä¿¡æ¯çš„é¡¶ç‚¹ç‰¹å¾ç»„æˆã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆæ¸²æŸ“å…·æœ‰é«˜å‡ ä½•ç»†èŠ‚çš„å¤æ‚åŸå¸‚ç¯å¢ƒã€‚

### ğŸ”‘ æ ¸å¿ƒç‰¹æ€§

- **ğŸ™ï¸ åŸå¸‚è§„æ¨¡æ¸²æŸ“**: é’ˆå¯¹å¤§è§„æ¨¡åŸå¸‚åœºæ™¯å’Œè‡ªåŠ¨é©¾é©¶æ•°æ®é›†è¿›è¡Œä¼˜åŒ–
- **ğŸ”² ç½‘æ ¼åŸºå…ƒ**: å…·æœ‰å¯å­¦ä¹ å‡ ä½•å½¢çŠ¶çš„å¯å˜å½¢ç¥ç»ç½‘æ ¼åŸºå…ƒ
- **âš¡ åŸºäºå…‰æ …åŒ–**: å¿«é€Ÿ GPU å…‰æ …åŒ–è€Œéå…‰çº¿è¡Œè¿›
- **ğŸ§© è‡ªç¼–ç å™¨æ¶æ„**: ç”¨äºç´§å‡‘æ½œåœ¨è¡¨ç¤ºçš„ç½‘æ ¼è‡ªç¼–ç å™¨
- **ğŸ¯ ä¸¤é˜¶æ®µè®­ç»ƒ**: ç‹¬ç«‹çš„å‡ ä½•å’Œè¾å°„ä¼˜åŒ–
- **ğŸ“Š å¤šæ•°æ®é›†æ”¯æŒ**: KITTI-360ã€Waymo å’Œè‡ªå®šä¹‰åŸå¸‚æ•°æ®é›†

### ğŸ—ï¸ æ¶æ„ç»„ä»¶

1. **å¯å˜å½¢ç¥ç»ç½‘æ ¼åŸºå…ƒ (DNMP)**: å…·æœ‰å½¢çŠ¶æ½œåœ¨ç¼–ç çš„å¯å­¦ä¹ ç½‘æ ¼å•å…ƒ
2. **ç½‘æ ¼è‡ªç¼–ç å™¨**: ç”¨äºç½‘æ ¼å½¢çŠ¶è¡¨ç¤ºçš„ç¼–ç å™¨-è§£ç å™¨
3. **å…‰æ …åŒ–ç®¡é“**: GPU åŠ é€Ÿçš„ç½‘æ ¼æ¸²æŸ“
4. **è¾å°„ MLP**: ä»é¡¶ç‚¹ç‰¹å¾è¿›è¡Œè§†è§’ç›¸å…³çš„é¢œè‰²é¢„æµ‹
5. **ä½“ç´ ç½‘æ ¼ç®¡ç†**: åŸºå…ƒçš„ç©ºé—´ç»„ç»‡

## å®‰è£…

DNMP-NeRF æ˜¯ NeuroCity é¡¹ç›®çš„ä¸€éƒ¨åˆ†ã€‚ç¡®ä¿æ‚¨å…·æœ‰ä»¥ä¸‹ä¾èµ–é¡¹ï¼š

```bash
pip install torch torchvision numpy matplotlib opencv-python
pip install trimesh pymeshlab  # ç”¨äºç½‘æ ¼å¤„ç†
pip install nvdiffrast  # ç”¨äºå¯å¾®åˆ†å…‰æ …åŒ– (å¯é€‰)
```

## å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ç”¨æ³•

```python
from src.nerfs.dnmp_nerf import DNMP, DNMPConfig, MeshAutoEncoder

# åˆ›å»ºé…ç½®
config = DNMPConfig(
    primitive_resolution=32,
    latent_dim=128,
    vertex_feature_dim=64,
    voxel_size=2.0,
    scene_bounds=(-100, 100, -100, 100, -5, 15)
)

# åˆ›å»ºç½‘æ ¼è‡ªç¼–ç å™¨
mesh_autoencoder = MeshAutoEncoder(
    latent_dim=config.latent_dim,
    primitive_resolution=config.primitive_resolution
)

# åˆ›å»º DNMP æ¨¡å‹
model = DNMP(config, mesh_autoencoder)

# ä»ç‚¹äº‘åˆå§‹åŒ–åœºæ™¯
import torch
point_cloud = torch.randn(10000, 3) * 50  # éšæœºç‚¹
model.initialize_scene(point_cloud, voxel_size=2.0)

print(f"åˆå§‹åŒ–äº† {len(model.primitives)} ä¸ªç½‘æ ¼åŸºå…ƒ")
```

### è®­ç»ƒç¤ºä¾‹

```python
from src.nerfs.dnmp_nerf import (
    DNMPTrainer, TwoStageTrainer,
    UrbanSceneDataset, KITTI360Dataset
)

# è®¾ç½®æ•°æ®é›†
dataset = KITTI360Dataset(
    data_root="path/to/kitti360",
    sequence="00",
    frame_range=(0, 100),
    image_size=(1024, 384)
)

# ä¸¤é˜¶æ®µè®­ç»ƒ
trainer = TwoStageTrainer(
    model=model,
    dataset=dataset,
    config=config,
    geometry_epochs=50,    # é˜¶æ®µ 1: å‡ ä½•ä¼˜åŒ–
    radiance_epochs=100,   # é˜¶æ®µ 2: è¾å°„ä¼˜åŒ–
    batch_size=4096
)

# è®­ç»ƒ
trainer.train()
```

### æ¸²æŸ“

```python
from src.nerfs.dnmp_nerf import DNMPRasterizer

# è®¾ç½®å…‰æ …åŒ–å™¨
rasterizer = DNMPRasterizer(
    image_size=(1024, 384),
    near_plane=0.1,
    far_plane=100.0
)

# æ¸²æŸ“æ–°è§†è§’
camera_poses = torch.randn(10, 4, 4)  # éšæœºç›¸æœºå§¿æ€
intrinsics = torch.eye(3).unsqueeze(0).repeat(10, 1, 1)

for i, (pose, K) in enumerate(zip(camera_poses, intrinsics)):
    # ç”Ÿæˆå…‰çº¿
    rays_o, rays_d = generate_rays(pose, K, (1024, 384))
    
    # æ¸²æŸ“
    output = model(rays_o.reshape(-1, 3), rays_d.reshape(-1, 3), rasterizer)
    
    rgb = output['rgb'].reshape(384, 1024, 3)
    depth = output['depth'].reshape(384, 1024)
    
    # ä¿å­˜ç»“æœ
    save_image(rgb, f"render_{i:03d}.png")
    save_depth(depth, f"depth_{i:03d}.png")
```

## æ•°æ®é›†æ ¼å¼

### KITTI-360 æ•°æ®é›†

```python
from src.nerfs.dnmp_nerf import KITTI360Dataset

dataset = KITTI360Dataset(
    data_root="/path/to/KITTI-360",
    sequence="2013_05_28_drive_0000_sync",
    camera_id=0,  # å·¦ç›¸æœº
    frame_range=(0, 1000),
    image_size=(1408, 376)
)
```

### Waymo æ•°æ®é›†

```python
from src.nerfs.dnmp_nerf import WaymoDataset

dataset = WaymoDataset(
    data_root="/path/to/waymo",
    segment_name="segment-xxx",
    camera_name="FRONT",
    frame_range=(0, 200)
)
```

### è‡ªå®šä¹‰åŸå¸‚æ•°æ®é›†

```python
from src.nerfs.dnmp_nerf import UrbanSceneDataset

# æœŸæœ›çš„ç»“æ„:
# dataset/
# â”œâ”€â”€ images/
# â”‚   â”œâ”€â”€ 000000.png
# â”‚   â”œâ”€â”€ 000001.png
# â”‚   â””â”€â”€ ...
# â”œâ”€â”€ poses.txt      # ç›¸æœºå§¿æ€
# â”œâ”€â”€ intrinsics.txt # ç›¸æœºå†…å‚
# â””â”€â”€ point_cloud.ply # (å¯é€‰) LiDAR ç‚¹

dataset = UrbanSceneDataset(
    data_root="/path/to/custom/dataset",
    image_size=(1920, 1080),
    load_lidar=True
)
```

## é…ç½®é€‰é¡¹

### DNMPConfig

æ ¸å¿ƒæ¨¡å‹é…ç½®ï¼š

```python
config = DNMPConfig(
    # ç½‘æ ¼åŸºå…ƒè®¾ç½®
    primitive_resolution=32,        # æ¯ä¸ªåŸºå…ƒçš„ç½‘æ ¼åˆ†è¾¨ç‡
    latent_dim=128,                # æ½œåœ¨ç¼–ç ç»´åº¦
    vertex_feature_dim=64,         # é¡¶ç‚¹ç‰¹å¾ç»´åº¦
    
    # åœºæ™¯è®¾ç½®  
    voxel_size=2.0,               # åŸºå…ƒæ”¾ç½®çš„ä½“ç´ å¤§å°
    scene_bounds=(-100, 100, -100, 100, -5, 15),  # [x_min, x_max, y_min, y_max, z_min, z_max]
    
    # ç½‘ç»œæ¶æ„
    mlp_hidden_dim=256,           # MLP éšè—å±‚ç»´åº¦
    mlp_num_layers=4,             # MLP å±‚æ•°
    view_dependent=True,          # è§†è§’ç›¸å…³æ¸²æŸ“
    
    # æ¸²æŸ“è®¾ç½®
    max_ray_samples=64,           # æ¯æ¡å…‰çº¿çš„æœ€å¤§é‡‡æ ·æ•°
    near_plane=0.1,               # è¿‘è£å‰ªå¹³é¢
    far_plane=100.0,              # è¿œè£å‰ªå¹³é¢
    
    # è®­ç»ƒè®¾ç½®
    geometry_lr=1e-3,             # å‡ ä½•å­¦ä¹ ç‡
    radiance_lr=5e-4,             # è¾å°„å­¦ä¹ ç‡
    weight_decay=1e-4,            # æƒé‡è¡°å‡
    
    # æŸå¤±æƒé‡
    color_loss_weight=1.0,        # é¢œè‰²é‡å»ºæŸå¤±
    depth_loss_weight=0.1,        # æ·±åº¦ç›‘ç£æŸå¤±
    mesh_regularization_weight=0.01,      # ç½‘æ ¼å¹³æ»‘æ€§
    latent_regularization_weight=0.001    # æ½œåœ¨ç¼–ç æ­£åˆ™åŒ–
)
```

### RasterizationConfig

å…‰æ …åŒ–ç®¡é“è®¾ç½®ï¼š

```python
from src.nerfs.dnmp_nerf import RasterizationConfig

raster_config = RasterizationConfig(
    image_size=(1024, 768),       # è¾“å‡ºå›¾åƒåˆ†è¾¨ç‡
    tile_size=16,                 # å…‰æ …åŒ–ç“¦ç‰‡å¤§å°
    faces_per_pixel=8,            # æ¯åƒç´ æœ€å¤§é¢æ•°
    blur_radius=0.01,             # è½¯å…‰æ …åŒ–æ¨¡ç³Š
    depth_peeling=True,           # å¯ç”¨æ·±åº¦å‰¥ç¦»
    background_color=(0, 0, 0),   # èƒŒæ™¯é¢œè‰²
)
```

## å…³é”®ç®—æ³•

### ç½‘æ ¼åŸºå…ƒåˆå§‹åŒ–

DNMP åŸºäºç‚¹äº‘å¯†åº¦åˆå§‹åŒ–ç½‘æ ¼åŸºå…ƒï¼š

1. **ä½“ç´ ç½‘æ ¼åˆ›å»º**: å°†åœºæ™¯åˆ’åˆ†ä¸ºè§„åˆ™ä½“ç´ 
2. **å¯†åº¦ä¼°è®¡**: è®¡ç®—æ¯ä¸ªä½“ç´ çš„ç‚¹æ•°
3. **åŸºå…ƒæ”¾ç½®**: åœ¨é«˜å¯†åº¦ä½“ç´ ä¸­æ”¾ç½®åŸºå…ƒ
4. **å½¢çŠ¶åˆå§‹åŒ–**: ä»å±€éƒ¨å‡ ä½•åˆå§‹åŒ–æ½œåœ¨ç¼–ç 

### ä¸¤é˜¶æ®µè®­ç»ƒ

#### é˜¶æ®µ 1: å‡ ä½•ä¼˜åŒ–
- ä¼˜åŒ–ç½‘æ ¼æ½œåœ¨ç¼–ç å’Œé¡¶ç‚¹ä½ç½®
- ä½¿ç”¨æ¥è‡ª LiDAR/ç«‹ä½“è§†è§‰çš„æ·±åº¦ç›‘ç£
- åº”ç”¨ç½‘æ ¼æ­£åˆ™åŒ– (å¹³æ»‘æ€§ã€ä½“ç§¯ä¿æŒ)

#### é˜¶æ®µ 2: è¾å°„ä¼˜åŒ–  
- å›ºå®šå‡ ä½•ï¼Œä¼˜åŒ–é¡¶ç‚¹ç‰¹å¾å’Œè¾å°„ MLP
- ä½¿ç”¨æ¥è‡ª RGB å›¾åƒçš„å…‰åº¦æŸå¤±
- åº”ç”¨è§†è§’ç›¸å…³ç€è‰²

### å¯å¾®åˆ†å…‰æ …åŒ–

```python
# å…‰æ …åŒ–è¿‡ç¨‹çš„ä¼ªä»£ç 
def rasterize_primitives(primitives, camera_params):
    all_vertices = []
    all_faces = []
    all_features = []
    
    for primitive in primitives:
        vertices, faces, features = primitive()
        
        # è½¬æ¢åˆ°ç›¸æœºåæ ‡ç³»
        vertices_cam = transform_vertices(vertices, camera_params)
        
        all_vertices.append(vertices_cam)
        all_faces.append(faces + len(all_vertices))
        all_features.append(features)
    
    # å…‰æ …åŒ–ç»„åˆç½‘æ ¼
    fragments = rasterize_meshes(
        vertices=torch.cat(all_vertices),
        faces=torch.cat(all_faces),
        image_size=image_size
    )
    
    # æ’å€¼é¡¶ç‚¹ç‰¹å¾
    interpolated_features = interpolate_vertex_attributes(
        fragments, torch.cat(all_features)
    )
    
    return fragments, interpolated_features
```

## æ€§èƒ½

### æ¸²æŸ“é€Ÿåº¦

- **å®æ—¶æ¸²æŸ“**: åœ¨ 1024x768 åˆ†è¾¨ç‡ä¸‹ 30+ FPS
- **GPU å†…å­˜**: å…¸å‹åŸå¸‚åœºæ™¯çº¦ 4GB
- **è®­ç»ƒæ—¶é—´**: RTX 3090 ä¸Š KITTI-360 åºåˆ—éœ€ 2-4 å°æ—¶

### è´¨é‡æŒ‡æ ‡

æ¥è‡ªè®ºæ–‡åœ¨ KITTI-360 ä¸Šçš„ç»“æœï¼š
- **PSNR**: 25.2 dB (vs NeRF çš„ 23.8 dB)
- **SSIM**: 0.82 (vs NeRF çš„ 0.79)  
- **LPIPS**: 0.15 (vs NeRF çš„ 0.18)
- **æ¸²æŸ“é€Ÿåº¦**: æ¯” NeRF å¿« 50 å€

## å®ç”¨å·¥å…·

### ç½‘æ ¼å¤„ç†

```python
from src.nerfs.dnmp_nerf.utils import mesh_utils

# ä»è®­ç»ƒæ¨¡å‹æå–ç½‘æ ¼
mesh = mesh_utils.extract_scene_mesh(model)
mesh_utils.save_mesh(mesh, "scene_mesh.ply")

# ç½‘æ ¼è´¨é‡åˆ†æ
stats = mesh_utils.analyze_mesh_quality(mesh)
print(f"é¡¶ç‚¹æ•°: {stats['num_vertices']}")
print(f"é¢æ•°: {stats['num_faces']}")
print(f"æ°´å¯†æ€§: {stats['is_watertight']}")
```

### å‡ ä½•å·¥å…·

```python
from src.nerfs.dnmp_nerf.utils import geometry_utils

# ä½“ç´ ç½‘æ ¼æ“ä½œ
voxel_grid = geometry_utils.create_voxel_grid(
    point_cloud, voxel_size=2.0
)

occupied_voxels = geometry_utils.get_occupied_voxels(
    voxel_grid, min_points=10
)
```

### è¯„ä¼°æŒ‡æ ‡

```python
from src.nerfs.dnmp_nerf.utils import evaluation_utils

# è®¡ç®—æ¸²æŸ“æŒ‡æ ‡
metrics = evaluation_utils.compute_image_metrics(
    pred_images=rendered_images,
    gt_images=ground_truth_images
)

print(f"PSNR: {metrics['psnr']:.2f}")
print(f"SSIM: {metrics['ssim']:.3f}")
print(f"LPIPS: {metrics['lpips']:.3f}")

# å‡ ä½•è¯„ä¼°
geo_metrics = evaluation_utils.evaluate_geometry(
    pred_depth=predicted_depth,
    gt_depth=lidar_depth,
    mask=valid_mask
)
```

## é«˜çº§ç‰¹æ€§

### è‡ªå®šä¹‰ç½‘æ ¼æ‹“æ‰‘

```python
# å®šä¹‰è‡ªå®šä¹‰åŸºå…ƒæ‹“æ‰‘
class SpherePrimitive(DeformableNeuralMeshPrimitive):
    def _generate_base_faces(self, resolution):
        # ç”ŸæˆäºŒåé¢çƒæ‹“æ‰‘
        return generate_icosphere_faces(resolution)

# ä½¿ç”¨è‡ªå®šä¹‰åŸºå…ƒ
config.primitive_type = "sphere"
```

### å¤šå°ºåº¦è¡¨ç¤º

```python
config = DNMPConfig(
    primitive_resolution=[16, 32, 64],  # å¤šåˆ†è¾¨ç‡åŸºå…ƒ
    adaptive_subdivision=True,          # è‡ªé€‚åº”ç½‘æ ¼ç»†åˆ†
    subdivision_threshold=0.1           # ç»†åˆ†è¯¯å·®é˜ˆå€¼
)
```

### æ—¶é—´ä¸€è‡´æ€§

```python
# ç”¨äºåŠ¨æ€åœºæ™¯
config.temporal_smoothness_weight = 0.01
config.optical_flow_weight = 0.05

trainer = TemporalDNMPTrainer(
    model=model,
    dataset=video_dataset,
    config=config
)
```

## ç¤ºä¾‹

è¿è¡Œç¤ºä¾‹è„šæœ¬æŸ¥çœ‹ DNMP-NeRF çš„å®é™…æ•ˆæœï¼š

```bash
# åŸºæœ¬æ¼”ç¤º
python -m src.nerfs.dnmp_nerf.examples.basic_demo

# KITTI-360 è®­ç»ƒ
python -m src.nerfs.dnmp_nerf.examples.kitti360_training

# Waymo æ•°æ®é›†
python -m src.nerfs.dnmp_nerf.examples.waymo_demo

# è‡ªå®šä¹‰æ•°æ®é›†å‡†å¤‡
python -m src.nerfs.dnmp_nerf.examples.prepare_dataset
```

## é™åˆ¶

- **å†…å­˜ä½¿ç”¨**: å¤§å‹åœºæ™¯éœ€è¦å¤§é‡ GPU å†…å­˜
- **åˆå§‹åŒ–**: è´¨é‡ä¾èµ–äºè‰¯å¥½çš„ç‚¹äº‘åˆå§‹åŒ–
- **æ‹“æ‰‘**: æ¯ç§åŸºå…ƒç±»å‹çš„å›ºå®šç½‘æ ¼æ‹“æ‰‘
- **é€æ˜åº¦**: å¯¹é€æ˜/åŠé€æ˜æè´¨çš„æ”¯æŒæœ‰é™

## æœªæ¥å·¥ä½œ

- **åŠ¨æ€åœºæ™¯**: æ‰©å±•åˆ°ç§»åŠ¨å¯¹è±¡å’Œå˜å½¢
- **æè´¨å±æ€§**: æ”¯æŒ PBR æè´¨å’Œå…‰ç…§
- **å‹ç¼©**: ç”¨äºç§»åŠ¨éƒ¨ç½²çš„ç½‘æ ¼å’Œç‰¹å¾å‹ç¼©
- **å®æ—¶ç¼–è¾‘**: äº¤äº’å¼åœºæ™¯ç¼–è¾‘åŠŸèƒ½

## å¼•ç”¨

```bibtex
@inproceedings{dnmp2023,
  title={Urban Radiance Field Representation with Deformable Neural Mesh Primitives},
  author={Author, Name and Others},
  booktitle={IEEE International Conference on Computer Vision (ICCV)},
  year={2023}
}
```

## å‚è€ƒæ–‡çŒ®

- [DNMP è®ºæ–‡](https://arxiv.org/abs/xxxx.xxxxx)
- [NeRF: Representing Scenes as Neural Radiance Fields](https://arxiv.org/abs/2003.08934)
- [Neural Radiance Fields for Outdoor Scene Relighting](https://arxiv.org/abs/2112.05140)
- [KITTI-360 æ•°æ®é›†](http://www.cvlibs.net/datasets/kitti-360/)
- [Waymo å¼€æ”¾æ•°æ®é›†](https://waymo.com/open/) 