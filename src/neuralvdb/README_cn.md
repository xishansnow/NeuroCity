# NeuralVDB: é«˜æ•ˆç¨€ç–ä½“ç§¯ç¥ç»è¡¨ç¤º

NeuralVDBæ˜¯ä¸€ä¸ªé«˜æ•ˆçš„ç¨€ç–ä½“ç´ ç¥ç»è¡¨ç¤ºåº“ï¼ŒåŸºäºå…«å‰æ ‘æ•°æ®ç»“æ„å’Œç¥ç»ç½‘ç»œç¼–ç ï¼Œä¸“ä¸ºå¤§è§„æ¨¡åŸå¸‚åœºæ™¯å’Œå¤æ‚å‡ ä½•ä½“çš„å»ºæ¨¡è€Œè®¾è®¡ã€‚

## ç‰¹æ€§

### æ ¸å¿ƒåŠŸèƒ½
- **ç¨€ç–ä½“ç´ è¡¨ç¤º**: åŸºäºå…«å‰æ ‘çš„é«˜æ•ˆå­˜å‚¨ç»“æ„
- **ç¥ç»ç½‘ç»œç¼–ç **: ä½¿ç”¨MLPç½‘ç»œè¿›è¡Œç‰¹å¾æå–å’Œå ç”¨é¢„æµ‹
- **åˆ†å±‚æ•°æ®ç»“æ„**: æ”¯æŒè‡ªé€‚åº”åˆ†è¾¨ç‡å’Œå¤šå°ºåº¦å¤„ç†
- **å†…å­˜ä¼˜åŒ–**: ç›¸æ¯”ä¼ ç»Ÿä½“ç´ åŒ–æ–¹æ³•æ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨

### é«˜çº§åŠŸèƒ½
- **è‡ªé€‚åº”åˆ†è¾¨ç‡**: æ ¹æ®åœºæ™¯å¤æ‚åº¦åŠ¨æ€è°ƒæ•´ç»†åˆ†çº§åˆ«
- **å¤šå°ºåº¦ç‰¹å¾**: æ”¯æŒä¸åŒå°ºåº¦çš„ç‰¹å¾æå–å’Œèåˆ
- **æ¸è¿›å¼è®­ç»ƒ**: é€æ­¥å¢åŠ è®­ç»ƒå¤æ‚åº¦ä»¥è·å¾—æ›´å¥½çš„æ”¶æ•›
- **ç‰¹å¾å‹ç¼©**: ä½¿ç”¨é‡åŒ–å’Œèšç±»æŠ€æœ¯å‡å°‘å­˜å‚¨éœ€æ±‚

### åº”ç”¨åœºæ™¯
- **åŸå¸‚åœºæ™¯å»ºæ¨¡**: å¤§è§„æ¨¡åŸå¸‚ç¯å¢ƒçš„ç¨€ç–è¡¨ç¤º
- **SDF/å ç”¨åœºå»ºæ¨¡**: æ”¯æŒæœ‰ç¬¦å·è·ç¦»åœºå’Œå ç”¨åœºé¢„æµ‹
- **3Dé‡å»º**: ä»ç‚¹äº‘æ•°æ®é‡å»ºä¸‰ç»´åœºæ™¯
- **å®æ—¶æ¸²æŸ“**: é«˜æ•ˆçš„ä½“ç§¯æ¸²æŸ“å’Œå¯è§†åŒ–

## å®‰è£…

### ä¾èµ–è¦æ±‚
```bash
torch>=1.9.0
numpy>=1.19.0
scipy>=1.6.0
scikit-learn>=0.24.0
matplotlib>=3.3.0
tqdm>=4.60.0
```

### å®‰è£…æ–¹æ³•
```bash
# ä»æºç å®‰è£…
git clone <repository-url>
cd NeuroCity
pip install -e .

# æˆ–è€…ç›´æ¥å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

## å¿«é€Ÿå¼€å§‹

### åŸºç¡€ä½¿ç”¨

```python
import numpy as np
from neuralvdb import NeuralVDB, NeuralVDBConfig, create_sample_data

# åˆ›å»ºé…ç½®
config = NeuralVDBConfig(
    feature_dim=32,
    max_depth=8,
    learning_rate=1e-3
)

# åˆ›å»ºæ¨¡å‹
model = NeuralVDB(config)

# ç”Ÿæˆç¤ºä¾‹æ•°æ®
points, occupancies = create_sample_data(n_points=10000, scene_type='mixed')

# è®­ç»ƒæ¨¡å‹
model.fit(points, occupancies, num_epochs=100)

# é¢„æµ‹
test_points = np.random.rand(1000, 3) * 100
predictions = model.predict(test_points)
```

### é«˜çº§ä½¿ç”¨

```python
from neuralvdb import AdvancedNeuralVDB, AdvancedNeuralVDBConfig

# åˆ›å»ºé«˜çº§é…ç½®
config = AdvancedNeuralVDBConfig(
    feature_dim=64,
    max_depth=10,
    adaptive_resolution=True,
    multi_scale_features=True,
    progressive_training=True,
    feature_compression=True
)

# åˆ›å»ºé«˜çº§æ¨¡å‹
model = AdvancedNeuralVDB(config)

# è®­ç»ƒï¼ˆæ”¯æŒæ›´å¤šé«˜çº§åŠŸèƒ½ï¼‰
model.fit(points, occupancies, num_epochs=200)
```

### ç“¦ç‰‡æ•°æ®å¤„ç†

```python
from neuralvdb import TileCityGenerator, TileDataset

# ç”ŸæˆåŸå¸‚ç“¦ç‰‡æ•°æ®
generator = TileCityGenerator(
    city_size=(10000, 10000, 100),
    tile_size=(1000, 1000),
    output_dir="city_tiles"
)
generator.generate_and_save_all_tiles(density='medium')

# åŠ è½½ç“¦ç‰‡æ•°æ®è¿›è¡Œè®­ç»ƒ
dataset = TileDataset("city_tiles")
# ... è®­ç»ƒä»£ç 
```

## å‘½ä»¤è¡Œå·¥å…·

### è®­ç»ƒè„šæœ¬

```bash
# åŸºç¡€è®­ç»ƒ
python -m neuralvdb.train_neuralvdb \
    --model-type basic \
    --data-type synthetic \
    --scene-type urban \
    --epochs 100 \
    --output-dir ./outputs

# é«˜çº§è®­ç»ƒ
python -m neuralvdb.train_neuralvdb \
    --model-type advanced \
    --data-type tiles \
    --data-path ./city_tiles \
    --epochs 200 \
    --feature-dim 64 \
    --save-visualizations
```

### æ•°æ®ç”Ÿæˆ

```python
from neuralvdb import generate_sample_dataset

# ç”Ÿæˆç¤ºä¾‹æ•°æ®é›†
generate_sample_dataset(
    output_dir="./dataset",
    num_samples=1000,
    grid_size=(64, 64, 64),
    scene_types=['mixed', 'architectural', 'organic']
)
```

## API å‚è€ƒ

### æ ¸å¿ƒç±»

#### NeuralVDB
åŸºç¡€NeuralVDBæ¨¡å‹ç±»ã€‚

**æ–¹æ³•:**
- `fit(points, occupancies, ...)`: è®­ç»ƒæ¨¡å‹
- `predict(points)`: é¢„æµ‹å ç”¨æ¦‚ç‡
- `save(path)`: ä¿å­˜æ¨¡å‹
- `load(path)`: åŠ è½½æ¨¡å‹
- `visualize_octree(...)`: å¯è§†åŒ–å…«å‰æ ‘ç»“æ„

#### AdvancedNeuralVDB
é«˜çº§NeuralVDBæ¨¡å‹ç±»ï¼ŒåŒ…å«æ›´å¤šåŠŸèƒ½ã€‚

**é¢å¤–æ–¹æ³•:**
- `get_memory_usage()`: è·å–å†…å­˜ä½¿ç”¨ç»Ÿè®¡
- `optimize_structure()`: ä¼˜åŒ–å…«å‰æ ‘ç»“æ„
- `compress_features()`: å‹ç¼©ç‰¹å¾è¡¨ç¤º

### é…ç½®ç±»

#### NeuralVDBConfig
```python
@dataclass
class NeuralVDBConfig:
    voxel_size: float = 1.0              # ä½“ç´ å¤§å°
    max_depth: int = 8                   # æœ€å¤§å…«å‰æ ‘æ·±åº¦
    min_depth: int = 3                   # æœ€å°å…«å‰æ ‘æ·±åº¦
    feature_dim: int = 32                # ç‰¹å¾ç»´åº¦
    hidden_dims: List[int] = None        # éšè—å±‚ç»´åº¦
    activation: str = 'relu'             # æ¿€æ´»å‡½æ•°
    dropout: float = 0.1                 # Dropoutç‡
    learning_rate: float = 1e-3          # å­¦ä¹ ç‡
    weight_decay: float = 1e-5           # æƒé‡è¡°å‡
    batch_size: int = 1024               # æ‰¹é‡å¤§å°
    sparsity_threshold: float = 0.01     # ç¨€ç–æ€§é˜ˆå€¼
    occupancy_threshold: float = 0.5     # å ç”¨é˜ˆå€¼
```

#### AdvancedNeuralVDBConfig

ç»§æ‰¿è‡ªNeuralVDBConfigï¼Œå¢åŠ é«˜çº§å‚æ•°ï¼š

```python
adaptive_resolution: bool = True         # è‡ªé€‚åº”åˆ†è¾¨ç‡
multi_scale_features: bool = True        # å¤šå°ºåº¦ç‰¹å¾
progressive_training: bool = True        # æ¸è¿›å¼è®­ç»ƒ
feature_compression: bool = True         # ç‰¹å¾å‹ç¼©
compression_ratio: float = 0.5           # å‹ç¼©æ¯”
quantization_bits: int = 8               # é‡åŒ–ä½æ•°
clustering_method: str = 'kmeans'        # èšç±»æ–¹æ³•
```

### æ•°æ®é›†ç±»

#### VDBDataset
```python
from neuralvdb import VDBDataset

dataset = VDBDataset(
    data_path="path/to/data",
    split='train',
    cache_data=True,
    transform=None
)
```

#### TileDataset
```python
from neuralvdb import TileDataset

dataset = TileDataset(
    tiles_dir="path/to/tiles",
    sample_ratio=0.1,
    stratified_sampling=True
)
```

## é«˜çº§åŠŸèƒ½

### è‡ªé€‚åº”å…«å‰æ ‘ç»“æ„

```python
# å¯ç”¨è‡ªé€‚åº”åˆ†è¾¨ç‡
config = AdvancedNeuralVDBConfig(
    adaptive_resolution=True,
    adaptation_threshold=0.02,
    max_subdivision_level=10
)

model = AdvancedNeuralVDB(config)
```

### å¤šå°ºåº¦ç‰¹å¾èåˆ

```python
# å¤šå°ºåº¦ç‰¹å¾é…ç½®
config = AdvancedNeuralVDBConfig(
    multi_scale_features=True,
    scale_levels=[1, 2, 4, 8],
    feature_fusion_method='attention'
)
```

### æ¸è¿›å¼è®­ç»ƒ

```python
# æ¸è¿›å¼è®­ç»ƒç­–ç•¥
training_schedule = {
    'epochs': [50, 100, 150, 200],
    'depths': [4, 6, 8, 10],
    'learning_rates': [1e-2, 5e-3, 1e-3, 5e-4]
}

model.fit_progressive(
    points, occupancies,
    schedule=training_schedule
)
```

## æ€§èƒ½ä¼˜åŒ–

### å†…å­˜ä¼˜åŒ–

```python
# å¯ç”¨ç‰¹å¾å‹ç¼©
config = AdvancedNeuralVDBConfig(
    feature_compression=True,
    compression_ratio=0.5,
    quantization_bits=8
)

# ä½¿ç”¨æµå¼æ•°æ®åŠ è½½
from neuralvdb import StreamingDataLoader

dataloader = StreamingDataLoader(
    dataset,
    batch_size=1024,
    prefetch_factor=2,
    num_workers=4
)
```

### è®­ç»ƒåŠ é€Ÿ

```python
# æ··åˆç²¾åº¦è®­ç»ƒ
from neuralvdb import NeuralVDBTrainer

trainer = NeuralVDBTrainer(
    model=model,
    use_amp=True,
    gradient_accumulation_steps=4
)

# å¤šGPUè®­ç»ƒ
trainer = NeuralVDBTrainer(
    model=model,
    device_ids=[0, 1, 2, 3],
    distributed=True
)
```

## å¯è§†åŒ–å·¥å…·

### å…«å‰æ ‘å¯è§†åŒ–

```python
from neuralvdb.visualization import visualize_octree

# å¯è§†åŒ–å…«å‰æ ‘ç»“æ„
visualize_octree(
    model.octree,
    depth_range=(3, 8),
    show_features=True,
    output_path="octree_vis.png"
)
```

### ç‰¹å¾å¯è§†åŒ–

```python
from neuralvdb.visualization import visualize_features

# å¯è§†åŒ–å­¦ä¹ åˆ°çš„ç‰¹å¾
visualize_features(
    model,
    sample_points=test_points,
    feature_dim_to_show=[0, 1, 2],
    output_path="features_vis.png"
)
```

### 3Dåœºæ™¯æ¸²æŸ“

```python
from neuralvdb.viewer import VDBViewer

# å¯åŠ¨äº¤äº’å¼æŸ¥çœ‹å™¨
viewer = VDBViewer(model)
viewer.launch(
    port=8080,
    resolution=(1024, 768),
    camera_controls=True
)
```

## è¯„ä¼°æŒ‡æ ‡

### å‡ ä½•ç²¾åº¦

```python
from neuralvdb.metrics import evaluate_geometry

metrics = evaluate_geometry(
    model=model,
    ground_truth_points=gt_points,
    ground_truth_occupancies=gt_occupancies
)

print(f"IoU: {metrics['iou']:.3f}")
print(f"Chamfer Distance: {metrics['chamfer_distance']:.6f}")
print(f"F-Score: {metrics['f_score']:.3f}")
```

### å‹ç¼©æ•ˆç‡

```python
from neuralvdb.metrics import evaluate_compression

compression_metrics = evaluate_compression(model)
print(f"å‹ç¼©æ¯”: {compression_metrics['compression_ratio']:.2f}")
print(f"é‡å»ºè¯¯å·®: {compression_metrics['reconstruction_error']:.6f}")
```

## åº”ç”¨ç¤ºä¾‹

### åŸå¸‚å»ºç­‘å»ºæ¨¡

```python
# åŠ è½½åŸå¸‚å»ºç­‘æ•°æ®
from neuralvdb.datasets import CityBuildingDataset

dataset = CityBuildingDataset(
    city_bounds=(-1000, -1000, 0, 1000, 1000, 200),
    building_types=['residential', 'commercial', 'industrial']
)

# è®­ç»ƒä¸“é—¨çš„åŸå¸‚æ¨¡å‹
config = NeuralVDBConfig(
    max_depth=10,
    feature_dim=64,
    sparsity_threshold=0.005
)

model = NeuralVDB(config)
model.fit(dataset.points, dataset.occupancies)
```

### åŒ»å­¦å›¾åƒåˆ†å‰²

```python
# åŒ»å­¦æ•°æ®é€‚é…
from neuralvdb.medical import MedicalVolumeAdapter

adapter = MedicalVolumeAdapter()
points, labels = adapter.convert_dicom_to_points("ct_scan.dcm")

# è®­ç»ƒåˆ†å‰²æ¨¡å‹
segmentation_model = NeuralVDB(NeuralVDBConfig(
    feature_dim=32,
    max_depth=8,
    occupancy_threshold=0.3
))
```

### ç‚¹äº‘é‡å»º

```python
# ä»ç‚¹äº‘é‡å»ºè¡¨é¢
from neuralvdb.reconstruction import PointCloudReconstructor

reconstructor = PointCloudReconstructor(
    model=model,
    surface_threshold=0.5,
    smoothing_iterations=10
)

mesh = reconstructor.extract_mesh(point_cloud)
```

## æ€§èƒ½åŸºå‡†

### å†…å­˜ä½¿ç”¨å¯¹æ¯”

| æ–¹æ³• | 1Mä½“ç´  | 10Mä½“ç´  | 100Mä½“ç´  |
|------|---------|---------|----------|
| ä¼ ç»Ÿä½“ç´ ç½‘æ ¼ | 4GB | 40GB | 400GB |
| å…«å‰æ ‘ | 0.8GB | 5GB | 35GB |
| NeuralVDB | 0.2GB | 1.2GB | 8GB |

### è®­ç»ƒæ—¶é—´

| åœºæ™¯å¤æ‚åº¦ | ä¼ ç»Ÿæ–¹æ³• | NeuralVDB | åŠ é€Ÿæ¯” |
|------------|----------|-----------|--------|
| ç®€å• | 2å°æ—¶ | 30åˆ†é’Ÿ | 4x |
| ä¸­ç­‰ | 8å°æ—¶ | 1.5å°æ—¶ | 5.3x |
| å¤æ‚ | 24å°æ—¶ | 4å°æ—¶ | 6x |

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**å†…å­˜ä¸è¶³**
```python
# å‡å°‘æ‰¹é‡å¤§å°å’Œç‰¹å¾ç»´åº¦
config.batch_size = 512
config.feature_dim = 16
```

**è®­ç»ƒæ”¶æ•›æ…¢**
```python
# è°ƒæ•´å­¦ä¹ ç‡å’Œä½¿ç”¨warmup
config.learning_rate = 5e-3
config.warmup_steps = 1000
```

**å…«å‰æ ‘è¿‡æ·±**
```python
# é™åˆ¶æœ€å¤§æ·±åº¦
config.max_depth = 6
config.sparsity_threshold = 0.02
```

## æ‰©å±•å’Œè‡ªå®šä¹‰

### è‡ªå®šä¹‰æŸå¤±å‡½æ•°

```python
from neuralvdb.losses import BaseLoss

class CustomLoss(BaseLoss):
    def forward(self, predictions, targets):
        # å®ç°è‡ªå®šä¹‰æŸå¤±
        return loss_value
```

### è‡ªå®šä¹‰æ•°æ®åŠ è½½å™¨

```python
from neuralvdb.datasets import BaseDataset

class CustomDataset(BaseDataset):
    def __init__(self, data_path):
        # å®ç°è‡ªå®šä¹‰æ•°æ®åŠ è½½
        pass
    
    def __getitem__(self, idx):
        # è¿”å›æ•°æ®æ ·æœ¬
        return points, occupancies
```

## å¼€å‘è·¯çº¿å›¾

### å³å°†æ¨å‡ºçš„åŠŸèƒ½

- [ ] æ”¯æŒé¢œè‰²ä¿¡æ¯ç¼–ç 
- [ ] å®æ—¶äº¤äº’å¼ç¼–è¾‘
- [ ] åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–
- [ ] WebGLå¯è§†åŒ–æ¥å£
- [ ] ç§»åŠ¨ç«¯æ¨ç†æ”¯æŒ

### é•¿æœŸç›®æ ‡

- [ ] æ—¶é—´åºåˆ—æ”¯æŒï¼ˆ4Dï¼‰
- [ ] ç‰©ç†ä»¿çœŸé›†æˆ
- [ ] AR/VRåº”ç”¨æ”¯æŒ
- [ ] äº‘ç«¯æœåŠ¡éƒ¨ç½²

## è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿å„ç§å½¢å¼çš„è´¡çŒ®ï¼š

1. **BugæŠ¥å‘Š**ï¼šä½¿ç”¨GitHub Issues
2. **åŠŸèƒ½è¯·æ±‚**ï¼šåˆ›å»ºFeature Request
3. **ä»£ç è´¡çŒ®**ï¼šæäº¤Pull Request
4. **æ–‡æ¡£æ”¹è¿›**ï¼šä¿®æ”¹æˆ–æ·»åŠ æ–‡æ¡£
5. **ç¤ºä¾‹å’Œæ•™ç¨‹**ï¼šåˆ†äº«ä½¿ç”¨ç»éªŒ

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# å…‹éš†ä»“åº“
git clone <repository-url>
cd NeuroCity

# åˆ›å»ºå¼€å‘ç¯å¢ƒ
python -m venv dev_env
source dev_env/bin/activate

# å®‰è£…å¼€å‘ä¾èµ–
pip install -e ".[dev]"

# è¿è¡Œæµ‹è¯•
pytest tests/
```

## è®¸å¯è¯

MITè®¸å¯è¯ - è¯¦è§LICENSEæ–‡ä»¶

## å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨NeuralVDBï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{neuralvdb2024,
  title={NeuralVDB: Efficient Sparse Volumetric Neural Representations},
  author={NeuroCity Team},
  year={2024},
  url={https://github.com/neurocity/neuralvdb}
}
```

## è”ç³»æ–¹å¼

- ğŸ“§ é‚®ä»¶ï¼šneuralvdb@neurocity.ai
- ğŸ’¬ è®¨è®ºï¼šGitHub Discussions
- ğŸ› é—®é¢˜æŠ¥å‘Šï¼šGitHub Issues
- ğŸ“– æ–‡æ¡£ï¼šhttps://neuralvdb.readthedocs.io
- ğŸŒ å®˜ç½‘ï¼šhttps://neurocity.ai/neuralvdb

## è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®å’Œç ”ç©¶å·¥ä½œï¼š
- OpenVDBé¡¹ç›®æä¾›çš„æ•°æ®ç»“æ„çµæ„Ÿ
- PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
- æ‰€æœ‰è´¡çŒ®è€…å’Œç”¨æˆ·çš„æ”¯æŒ

---

**NeuralVDBï¼šè®©ç¨€ç–ä½“ç§¯è¡¨ç¤ºæ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆï¼** ğŸš€ 