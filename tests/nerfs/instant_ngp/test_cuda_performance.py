from __future__ import annotations

"""
Instant NGP CUDA æ€§èƒ½åŸºå‡†æµ‹è¯•

æµ‹è¯• CUDA æ‰©å±•çš„æ€§èƒ½ä¼˜åŠ¿ã€‚
"""

import pytest
import torch
import time
import sys
import os

sys.path.append(os.path.join(os.path.dirname(__file__), "..", "..", "..", "src"))

from nerfs.instant_ngp.core import InstantNGPConfig, InstantNGPModel, HashEncoder


class TestInstantNGPCUDAPerformance:
    """CUDA æ€§èƒ½æµ‹è¯•ç±»"""

    def setup_method(self):
        """æµ‹è¯•è®¾ç½®"""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.config = InstantNGPConfig(
            num_levels=8,
            base_resolution=16,
            finest_resolution=128,
            feature_dim=2,
            log2_hashmap_size=18,
            num_samples=64,
        )

    @pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
    def test_cuda_vs_cpu_performance(self):
        """æµ‹è¯• CUDA vs CPU æ€§èƒ½æ¯”è¾ƒ"""
        print(f"\nğŸƒ CUDA vs CPU æ€§èƒ½æ¯”è¾ƒ")
        print("=" * 50)
        
        # æµ‹è¯•æ•°æ®
        batch_sizes = [1000, 5000, 10000]
        
        for batch_size in batch_sizes:
            print(f"\nğŸ“Š æ‰¹æ¬¡å¤§å°: {batch_size}")
            
            # åˆ›å»ºæµ‹è¯•è¾“å…¥
            positions_cpu = torch.randn(batch_size, 3)
            positions_cuda = positions_cpu.to('cuda')
            
            # CPU æµ‹è¯•
            model_cpu = InstantNGPModel(self.config).to('cpu')
            model_cpu.eval()
            
            # é¢„çƒ­
            with torch.no_grad():
                _ = model_cpu.encoding(positions_cpu[:100])
            
            # CPU æ€§èƒ½æµ‹è¯•
            start_time = time.time()
            with torch.no_grad():
                for _ in range(10):
                    _ = model_cpu.encoding(positions_cpu)
            cpu_time = (time.time() - start_time) / 10
            
            # CUDA æµ‹è¯•
            model_cuda = InstantNGPModel(self.config).to('cuda')
            model_cuda.eval()
            
            # é¢„çƒ­
            with torch.no_grad():
                _ = model_cuda.encoding(positions_cuda[:100])
            torch.cuda.synchronize()
            
            # CUDA æ€§èƒ½æµ‹è¯•
            start_time = time.time()
            with torch.no_grad():
                for _ in range(10):
                    _ = model_cuda.encoding(positions_cuda)
            torch.cuda.synchronize()
            cuda_time = (time.time() - start_time) / 10
            
            # æ‰“å°ç»“æœ
            speedup = cpu_time / cuda_time if cuda_time > 0 else float('inf')
            print(f"  CPU æ—¶é—´:   {cpu_time*1000:.2f} ms")
            print(f"  CUDA æ—¶é—´:  {cuda_time*1000:.2f} ms")
            print(f"  åŠ é€Ÿæ¯”:     {speedup:.2f}x")
            
            # åŸºæœ¬æ–­è¨€
            assert cuda_time > 0
            assert cpu_time > 0
            # é€šå¸¸ CUDA åº”è¯¥æ¯” CPU å¿«ï¼Œä½†ä¸å¼ºåˆ¶è¦æ±‚
            if speedup < 1.0:
                print(f"  âš ï¸  è­¦å‘Š: CUDA æ¯” CPU æ…¢ (å¯èƒ½æ˜¯å°æ‰¹æ¬¡å¼€é”€)")

    @pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨æµ‹è¯•")
        print("=" * 50)
        
        # æ¸…ç©º CUDA ç¼“å­˜
        torch.cuda.empty_cache()
        initial_memory = torch.cuda.memory_allocated()
        
        # åˆ›å»ºæ¨¡å‹
        model = InstantNGPModel(self.config).to('cuda')
        model_memory = torch.cuda.memory_allocated() - initial_memory
        
        # æµ‹è¯•å‰å‘ä¼ æ’­å†…å­˜
        batch_size = 10000
        positions = torch.randn(batch_size, 3, device='cuda')
        
        forward_memory_start = torch.cuda.memory_allocated()
        with torch.no_grad():
            output = model.encoding(positions)
        forward_memory = torch.cuda.memory_allocated() - forward_memory_start
        
        print(f"æ¨¡å‹å†…å­˜:     {model_memory / 1024**2:.2f} MB")
        print(f"å‰å‘ä¼ æ’­å†…å­˜: {forward_memory / 1024**2:.2f} MB")
        print(f"è¾“å‡ºå½¢çŠ¶:     {output.shape}")
        
        # åŸºæœ¬æ£€æŸ¥
        assert model_memory > 0
        assert forward_memory >= 0
        assert output.shape[0] == batch_size

    @pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
    def test_batch_scaling(self):
        """æµ‹è¯•æ‰¹æ¬¡å¤§å°ç¼©æ”¾æ€§èƒ½"""
        print(f"\nğŸ“ˆ æ‰¹æ¬¡ç¼©æ”¾æ€§èƒ½æµ‹è¯•")
        print("=" * 50)
        
        model = InstantNGPModel(self.config).to('cuda')
        model.eval()
        
        batch_sizes = [100, 500, 1000, 5000, 10000]
        times = []
        
        for batch_size in batch_sizes:
            positions = torch.randn(batch_size, 3, device='cuda')
            
            # é¢„çƒ­
            with torch.no_grad():
                _ = model.encoding(positions[:min(100, batch_size)])
            torch.cuda.synchronize()
            
            # æ€§èƒ½æµ‹è¯•
            start_time = time.time()
            with torch.no_grad():
                for _ in range(5):
                    _ = model.encoding(positions)
            torch.cuda.synchronize()
            avg_time = (time.time() - start_time) / 5
            
            times.append(avg_time)
            throughput = batch_size / avg_time
            
            print(f"æ‰¹æ¬¡: {batch_size:5d}, æ—¶é—´: {avg_time*1000:6.2f} ms, "
                  f"ååé‡: {throughput:8.0f} samples/s")
        
        # æ£€æŸ¥æ€§èƒ½åˆç†æ€§
        assert all(t > 0 for t in times)
        # é€šå¸¸æ›´å¤§çš„æ‰¹æ¬¡åº”è¯¥æœ‰æ›´é«˜çš„ååé‡
        # ä½†ä¸å¼ºåˆ¶è¦æ±‚ï¼Œå› ä¸ºå¯èƒ½æœ‰å†…å­˜é™åˆ¶

    def test_model_consistency(self):
        """æµ‹è¯•æ¨¡å‹åœ¨ CPU å’Œ CUDA ä¸Šçš„ä¸€è‡´æ€§"""
        print(f"\nğŸ” CPU/CUDA ä¸€è‡´æ€§æµ‹è¯•")
        print("=" * 50)
        
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")
        
        # åˆ›å»ºç›¸åŒçš„æ¨¡å‹
        torch.manual_seed(42)
        model_cpu = InstantNGPModel(self.config).to('cpu')
        
        torch.manual_seed(42)
        model_cuda = InstantNGPModel(self.config).to('cuda')
        
        # å¤åˆ¶æƒé‡ç¡®ä¿ä¸€è‡´
        model_cuda.load_state_dict(model_cpu.state_dict())
        
        # æµ‹è¯•è¾“å…¥
        positions = torch.randn(100, 3)
        positions_cuda = positions.to('cuda')
        
        # å‰å‘ä¼ æ’­
        model_cpu.eval()
        model_cuda.eval()
        
        with torch.no_grad():
            output_cpu = model_cpu.encoding(positions)
            output_cuda = model_cuda.encoding(positions_cuda).cpu()
        
        # æ£€æŸ¥ä¸€è‡´æ€§
        max_diff = torch.max(torch.abs(output_cpu - output_cuda)).item()
        mean_diff = torch.mean(torch.abs(output_cpu - output_cuda)).item()
        
        print(f"æœ€å¤§å·®å¼‚: {max_diff:.6f}")
        print(f"å¹³å‡å·®å¼‚: {mean_diff:.6f}")
        
        # å…è®¸å°çš„æ•°å€¼è¯¯å·®
        assert max_diff < 1e-4, f"CPU/CUDA è¾“å‡ºå·®å¼‚è¿‡å¤§: {max_diff}"
        print("âœ… CPU/CUDA è¾“å‡ºä¸€è‡´")

    def teardown_method(self):
        """æ¸…ç†"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
