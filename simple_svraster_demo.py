"""
SVRaster ç®€åŒ–æ¨ç†æ¼”ç¤º

è¿™ä¸ªè„šæœ¬å±•ç¤ºSVRasterçš„æ ¸å¿ƒæ¨ç†åŠŸèƒ½ï¼Œä¸ä¾èµ–å¤æ‚çš„æ•°æ®é›†æ¨¡å—
"""

import torch
import numpy as np
import time
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/xishansnow/3DVision/NeuroCity')

# åªå¯¼å…¥æ ¸å¿ƒç»„ä»¶
from src.nerfs.svraster.core import SVRasterModel, SVRasterConfig


def simple_inference_demo():
    """ç®€åŒ–çš„æ¨ç†æ¼”ç¤º"""
    
    print("=== SVRaster ç®€åŒ–æ¨ç†æ¼”ç¤º ===\n")
    
    # 1. åˆ›å»ºé…ç½®
    print("1. åˆ›å»ºé…ç½®...")
    config = SVRasterConfig(
        image_width=400,
        image_height=300,
        scene_bounds=(-1.5, -1.5, -1.5, 1.5, 1.5, 1.5),
        sh_degree=1,
        grid_resolution=64  # è¾ƒå°çš„ç½‘æ ¼ç”¨äºå¿«é€Ÿæ¼”ç¤º
    )
    print(f"   - å›¾åƒåˆ†è¾¨ç‡: {config.image_width}x{config.image_height}")
    print(f"   - åœºæ™¯è¾¹ç•Œ: {config.scene_bounds}")
    print(f"   - ç½‘æ ¼åˆ†è¾¨ç‡: {config.grid_resolution}")
    
    # 2. åˆå§‹åŒ–æ¨¡å‹
    print("\n2. åˆå§‹åŒ–æ¨¡å‹...")
    model = SVRasterModel(config)
    print(f"   - æ¨¡å‹è®¾å¤‡: {model.device}")
    print(f"   - ä½“ç´ ç½‘æ ¼å½¢çŠ¶: {model.voxel_grid.shape}")
    print(f"   - ç‰¹å¾ç½‘æ ¼å½¢çŠ¶: {model.feature_grid.shape}")
    
    # 3. åˆ›å»ºæµ‹è¯•åœºæ™¯
    print("\n3. åˆ›å»ºæµ‹è¯•åœºæ™¯...")
    with torch.no_grad():
        # ç”Ÿæˆåæ ‡ç½‘æ ¼
        res = config.grid_resolution
        coords = torch.stack(torch.meshgrid(
            torch.linspace(-1, 1, res),
            torch.linspace(-1, 1, res),
            torch.linspace(-1, 1, res),
            indexing='ij'
        ), dim=-1)
        
        # åˆ›å»ºä¸¤ä¸ªçƒå½¢å¯¹è±¡
        center1 = torch.tensor([0.3, 0.0, 0.0])
        center2 = torch.tensor([-0.3, 0.0, 0.0])
        
        dist1 = torch.norm(coords - center1, dim=-1)
        dist2 = torch.norm(coords - center2, dim=-1)
        
        # é«˜æ–¯åˆ†å¸ƒå¯†åº¦
        density1 = torch.exp(-dist1 * 4.0) * 0.8
        density2 = torch.exp(-dist2 * 3.0) * 0.6
        
        # åˆå¹¶å¯†åº¦
        total_density = density1 + density2
        
        # è®¾ç½®ä½“ç´ ç½‘æ ¼
        model.voxel_grid = total_density.unsqueeze(-1).to(model.device)
        
        # åˆ›å»ºå½©è‰²ç‰¹å¾ï¼ˆè®©ä¸¤ä¸ªçƒæœ‰ä¸åŒé¢œè‰²ï¼‰
        features = torch.zeros(*coords.shape[:-1], model.feature_grid.shape[-1])
        
        # ç¬¬ä¸€ä¸ªçƒï¼ˆçº¢è‰²ï¼‰
        mask1 = dist1 < 0.5
        features[mask1, 0] = 1.0  # çº¢è‰²é€šé“
        
        # ç¬¬äºŒä¸ªçƒï¼ˆè“è‰²ï¼‰
        mask2 = dist2 < 0.5
        features[mask2, 2] = 1.0  # è“è‰²é€šé“
        
        # æ·»åŠ ä¸€äº›å™ªå£°
        features += torch.randn_like(features) * 0.1
        
        model.feature_grid = features.to(model.device)
    
    print(f"   - åˆ›å»ºäº†ä¸¤ä¸ªçƒå½¢å¯¹è±¡")
    print(f"   - å¯†åº¦èŒƒå›´: [{model.voxel_grid.min():.3f}, {model.voxel_grid.max():.3f}]")
    
    # 4. ç”Ÿæˆæµ‹è¯•å…‰çº¿
    print("\n4. ç”Ÿæˆæµ‹è¯•å…‰çº¿...")
    
    # ç›¸æœºä½ç½®
    camera_pos = torch.tensor([0.0, 0.0, 2.0], device=model.device)
    
    # ç”Ÿæˆå…‰çº¿ï¼ˆä»ç›¸æœºæŒ‡å‘åœºæ™¯ï¼‰
    num_rays = 1000
    
    # åœ¨å›¾åƒå¹³é¢ä¸Šéšæœºé‡‡æ ·
    pixel_coords = torch.rand(num_rays, 2, device=model.device) * 2 - 1  # [-1, 1]
    
    # å…‰çº¿èµ·ç‚¹ï¼ˆæ‰€æœ‰å…‰çº¿éƒ½ä»ç›¸æœºä½ç½®å¼€å§‹ï¼‰
    rays_o = camera_pos.unsqueeze(0).expand(num_rays, -1)
    
    # å…‰çº¿æ–¹å‘ï¼ˆä»ç›¸æœºæŒ‡å‘åœºæ™¯ä¸­çš„åƒç´ ï¼‰
    rays_d = torch.stack([
        pixel_coords[:, 0] * 0.8,  # xæ–¹å‘
        pixel_coords[:, 1] * 0.6,  # yæ–¹å‘
        torch.full((num_rays,), -1.0, device=model.device)  # zæ–¹å‘ï¼ˆå‘å‰ï¼‰
    ], dim=-1)
    
    # å½’ä¸€åŒ–æ–¹å‘
    rays_d = rays_d / torch.norm(rays_d, dim=1, keepdim=True)
    
    print(f"   - ç”Ÿæˆäº† {num_rays} æ¡å…‰çº¿")
    print(f"   - ç›¸æœºä½ç½®: {camera_pos}")
    print(f"   - å…‰çº¿èµ·ç‚¹å½¢çŠ¶: {rays_o.shape}")
    print(f"   - å…‰çº¿æ–¹å‘å½¢çŠ¶: {rays_d.shape}")
    
    # 5. è®­ç»ƒæ¨¡å¼æ¸²æŸ“ï¼ˆä½“ç§¯æ¸²æŸ“ï¼‰
    print("\n5. è®­ç»ƒæ¨¡å¼æ¸²æŸ“ï¼ˆä½“ç§¯æ¸²æŸ“ï¼‰...")
    
    start_time = time.time()
    with torch.no_grad():
        training_outputs = model(rays_o, rays_d, mode="training")
    training_time = time.time() - start_time
    
    print(f"   - æ¸²æŸ“æ—¶é—´: {training_time:.3f}ç§’")
    print(f"   - è¾“å‡ºé”®: {list(training_outputs.keys())}")
    if 'rgb' in training_outputs:
        rgb = training_outputs['rgb']
        print(f"   - RGBå½¢çŠ¶: {rgb.shape}")
        print(f"   - RGBèŒƒå›´: [{rgb.min():.3f}, {rgb.max():.3f}]")
    if 'depth' in training_outputs:
        depth = training_outputs['depth']
        print(f"   - æ·±åº¦å½¢çŠ¶: {depth.shape}")
        print(f"   - æ·±åº¦èŒƒå›´: [{depth.min():.3f}, {depth.max():.3f}]")
    
    # 6. æ¨ç†æ¨¡å¼æ¸²æŸ“ï¼ˆå…‰æ …åŒ–ï¼‰
    print("\n6. æ¨ç†æ¨¡å¼æ¸²æŸ“ï¼ˆå…‰æ …åŒ–ï¼‰...")
    
    start_time = time.time()
    with torch.no_grad():
        inference_outputs = model(rays_o, rays_d, mode="inference")
    inference_time = time.time() - start_time
    
    print(f"   - æ¸²æŸ“æ—¶é—´: {inference_time:.3f}ç§’")
    print(f"   - è¾“å‡ºé”®: {list(inference_outputs.keys())}")
    if 'rgb' in inference_outputs:
        rgb = inference_outputs['rgb']
        print(f"   - RGBå½¢çŠ¶: {rgb.shape}")
        print(f"   - RGBèŒƒå›´: [{rgb.min():.3f}, {rgb.max():.3f}]")
    if 'depth' in inference_outputs:
        depth = inference_outputs['depth']
        print(f"   - æ·±åº¦å½¢çŠ¶: {depth.shape}")
        print(f"   - æ·±åº¦èŒƒå›´: [{depth.min():.3f}, {depth.max():.3f}]")
    
    # 7. æ¯”è¾ƒç»“æœ
    print("\n7. æ¯”è¾ƒä¸¤ç§æ¸²æŸ“æ¨¡å¼...")
    
    if 'rgb' in training_outputs and 'rgb' in inference_outputs:
        rgb_diff = torch.mean(torch.abs(training_outputs['rgb'] - inference_outputs['rgb']))
        print(f"   - RGBå¹³å‡å·®å¼‚: {rgb_diff:.6f}")
    
    if 'depth' in training_outputs and 'depth' in inference_outputs:
        depth_diff = torch.mean(torch.abs(training_outputs['depth'] - inference_outputs['depth']))
        print(f"   - æ·±åº¦å¹³å‡å·®å¼‚: {depth_diff:.6f}")
    
    if training_time > 0 and inference_time > 0:
        speedup = training_time / inference_time
        print(f"   - é€Ÿåº¦æå‡: {speedup:.2f}x")
    
    # 8. æ€§èƒ½æµ‹è¯•
    print("\n8. æ€§èƒ½æµ‹è¯•...")
    
    batch_sizes = [100, 500, 1000]
    
    for batch_size in batch_sizes:
        # ç”Ÿæˆæµ‹è¯•å…‰çº¿
        test_rays_o = torch.randn(batch_size, 3, device=model.device) * 0.1
        test_rays_d = torch.randn(batch_size, 3, device=model.device)
        test_rays_d = test_rays_d / torch.norm(test_rays_d, dim=1, keepdim=True)
        
        # æ¨ç†æ¨¡å¼æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        with torch.no_grad():
            _ = model(test_rays_o, test_rays_d, mode="inference")
        elapsed = time.time() - start_time
        
        rays_per_sec = batch_size / elapsed if elapsed > 0 else float('inf')
        print(f"   - æ‰¹é‡å¤§å° {batch_size}: {elapsed:.3f}ç§’, {rays_per_sec:.0f} å…‰çº¿/ç§’")
    
    print("\n=== æ¼”ç¤ºå®Œæˆ ===")
    
    return training_outputs, inference_outputs


def demo_usage_guide():
    """æ¼”ç¤ºä½¿ç”¨æŒ‡å—"""
    
    print("\n=== SVRaster ä½¿ç”¨æŒ‡å— ===\n")
    
    print("1. åŸºæœ¬ä½¿ç”¨æµç¨‹:")
    print("   a) åˆ›å»ºé…ç½®: config = SVRasterConfig(...)")
    print("   b) åˆå§‹åŒ–æ¨¡å‹: model = SVRasterModel(config)")
    print("   c) è®­ç»ƒæ¨¡å¼: outputs = model(rays_o, rays_d, mode='training')")
    print("   d) æ¨ç†æ¨¡å¼: outputs = model(rays_o, rays_d, mode='inference')")
    
    print("\n2. å…³é”®å‚æ•°è¯´æ˜:")
    print("   - image_width/height: å›¾åƒåˆ†è¾¨ç‡")
    print("   - scene_bounds: åœºæ™¯è¾¹ç•Œ (x_min, y_min, z_min, x_max, y_max, z_max)")
    print("   - grid_resolution: ä½“ç´ ç½‘æ ¼åˆ†è¾¨ç‡")
    print("   - sh_degree: çƒè°å‡½æ•°é˜¶æ•°")
    
    print("\n3. è¾“å…¥æ•°æ®æ ¼å¼:")
    print("   - rays_o: å…‰çº¿èµ·ç‚¹ [N, 3]")
    print("   - rays_d: å…‰çº¿æ–¹å‘ [N, 3] (éœ€è¦å½’ä¸€åŒ–)")
    
    print("\n4. è¾“å‡ºæ•°æ®æ ¼å¼:")
    print("   - rgb: é¢œè‰² [N, 3]")
    print("   - depth: æ·±åº¦ [N]")
    print("   - alpha: é€æ˜åº¦ [N] (å¦‚æœå¯ç”¨)")
    
    print("\n5. ä¸¤ç§æ¸²æŸ“æ¨¡å¼:")
    print("   - training: ä½“ç§¯æ¸²æŸ“ï¼Œç”¨äºè®­ç»ƒï¼Œè¾ƒæ…¢ä½†å‡†ç¡®")
    print("   - inference: å…‰æ …åŒ–æ¸²æŸ“ï¼Œç”¨äºæ¨ç†ï¼Œè¾ƒå¿«")
    
    print("\n6. æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
    print("   - ä½¿ç”¨è¾ƒå°çš„ grid_resolution è¿›è¡Œå¿«é€Ÿæµ‹è¯•")
    print("   - æ‰¹é‡å¤„ç†å…‰çº¿ä»¥æé«˜æ•ˆç‡")
    print("   - æ¨ç†æ—¶ä½¿ç”¨ torch.no_grad() å‡å°‘å†…å­˜ä½¿ç”¨")


if __name__ == "__main__":
    try:
        # è¿è¡Œç®€åŒ–æ¼”ç¤º
        training_outputs, inference_outputs = simple_inference_demo()
        
        # æ˜¾ç¤ºä½¿ç”¨æŒ‡å—
        demo_usage_guide()
        
        print("\nğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
