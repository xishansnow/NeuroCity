#!/usr/bin/env python3
"""
SVRaster è®­ç»ƒæ¼”ç¤º

è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºå¦‚ä½•ä½¿ç”¨ SVRaster è¿›è¡Œç¥ç»è¾å°„åœºè®­ç»ƒã€‚
åŒ…å«å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼šæ•°æ®åŠ è½½ã€æ¨¡å‹åˆå§‹åŒ–ã€è®­ç»ƒå¾ªç¯ã€æŸå¤±è®¡ç®—ç­‰ã€‚

ç‰¹ç‚¹ï¼š
- ä½¿ç”¨ VolumeRenderer è¿›è¡Œä½“ç§¯æ¸²æŸ“è®­ç»ƒ
- æ”¯æŒè‡ªé€‚åº”ç¨€ç–ä½“ç´ 
- çƒè°å‡½æ•°è§†è§’ç›¸å…³é¢œè‰²
- ç°ä»£ PyTorch è®­ç»ƒå¾ªç¯
- å®æ—¶è®­ç»ƒç›‘æ§
"""

from __future__ import annotations

import sys
import torch
import torch.nn.functional as F
import numpy as np
import time
import logging
from pathlib import Path
from typing import Dict, Tuple, Optional
import json
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'src'))

# SVRaster å¯¼å…¥
from src.nerfs.svraster import (
    SVRasterModel, SVRasterConfig, 
    SVRasterTrainer, SVRasterTrainerConfig,
    SVRasterDataset, SVRasterDatasetConfig,
    VolumeRenderer, SVRasterLoss
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SVRasterTrainingDemo:
    """SVRaster è®­ç»ƒæ¼”ç¤ºç±»"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # è®­ç»ƒé…ç½®
        self.config = self._create_config()
        self.model = None
        self.trainer = None
        self.dataset = None
        
    def _create_config(self) -> SVRasterConfig:
        """åˆ›å»º SVRaster é…ç½®"""
        config = SVRasterConfig(
            # åœºæ™¯è®¾ç½®
            image_width=400,
            image_height=300,
            scene_bounds=(-2.0, -2.0, -2.0, 2.0, 2.0, 2.0),
            
            # ä½“ç´ ç½‘æ ¼è®¾ç½®
            base_resolution=64,
            max_octree_levels=8,
            
            # æ¸²æŸ“è®¾ç½®
            ray_samples_per_voxel=8,
            depth_peeling_layers=4,
            morton_ordering=True,
            
            # å¤–è§‚è®¾ç½®
            sh_degree=2,
            color_activation="sigmoid",
            density_activation="exp",
            
            # è®­ç»ƒè®¾ç½®
            background_color=(0.0, 0.0, 0.0),
            near_plane=0.1,
            far_plane=10.0,
        )
        
        logger.info("SVRaster é…ç½®åˆ›å»ºå®Œæˆ")
        logger.info(f"  - åˆ†è¾¨ç‡: {config.image_width}x{config.image_height}")
        logger.info(f"  - åŸºç¡€ç½‘æ ¼: {config.base_resolution}^3")
        logger.info(f"  - çƒè°é˜¶æ•°: {config.sh_degree}")
        
        return config
        
    def _create_synthetic_dataset(self) -> SVRasterDataset:
        """åˆ›å»ºåˆæˆè®­ç»ƒæ•°æ®é›†"""
        logger.info("åˆ›å»ºåˆæˆæ•°æ®é›†...")
        
        # æ•°æ®é›†é…ç½®
        dataset_config = SVRasterDatasetConfig(
            data_dir="demo_data",
            image_width=self.config.image_width,
            image_height=self.config.image_height,
            train_split=0.8,
            val_split=0.2,
            test_split=0.0,
        )
        
        # ç”Ÿæˆåˆæˆåœºæ™¯æ•°æ®
        self._generate_synthetic_scene_data(dataset_config)
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = SVRasterDataset(dataset_config)
        
        logger.info(f"æ•°æ®é›†åˆ›å»ºå®Œæˆï¼Œè®­ç»ƒæ ·æœ¬: {len(dataset)}")
        return dataset
        
    def _generate_synthetic_scene_data(self, dataset_config: SVRasterDatasetConfig):
        """ç”Ÿæˆåˆæˆåœºæ™¯æ•°æ®"""
        import os
        import imageio
        
        # åˆ›å»ºæ•°æ®ç›®å½•
        data_dir = Path(dataset_config.data_dir)
        data_dir.mkdir(exist_ok=True)
        
        images_dir = data_dir / "images"
        images_dir.mkdir(exist_ok=True)
        
        poses_dir = data_dir / "poses"
        poses_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆç›¸æœºä½ç½®
        n_views = 60  # å›ºå®šä½¿ç”¨60ä¸ªè§†è§’è¿›è¡Œæ¼”ç¤º
        
        # çƒé¢ç›¸æœºåˆ†å¸ƒ
        phi = np.random.uniform(0, 2 * np.pi, n_views)
        theta = np.random.uniform(np.pi/6, np.pi/3, n_views)
        radius = 3.0
        
        camera_positions = np.stack([
            radius * np.sin(theta) * np.cos(phi),
            radius * np.sin(theta) * np.sin(phi),
            radius * np.cos(theta)
        ], axis=1)
        
        # ç”Ÿæˆå›¾åƒå’Œç›¸æœºå‚æ•°
        transforms = {
            "camera_angle_x": 0.8,
            "frames": []
        }
        
        for i in range(n_views):
            # ç›¸æœºæœå‘åœºæ™¯ä¸­å¿ƒ
            camera_pos = camera_positions[i]
            target = np.array([0.0, 0.0, 0.0])
            up = np.array([0.0, 0.0, 1.0])
            
            # è®¡ç®—å˜æ¢çŸ©é˜µ
            forward = target - camera_pos
            forward = forward / np.linalg.norm(forward)
            right = np.cross(forward, up)
            right = right / np.linalg.norm(right)
            up = np.cross(right, forward)
            
            transform_matrix = np.eye(4)
            transform_matrix[:3, 0] = right
            transform_matrix[:3, 1] = up
            transform_matrix[:3, 2] = -forward
            transform_matrix[:3, 3] = camera_pos
            
            # ç”Ÿæˆç®€å•çš„åˆæˆå›¾åƒï¼ˆå½©è‰²çƒä½“ï¼‰
            image = self._generate_synthetic_image(
                camera_pos, forward, 
                dataset_config.image_width, 
                dataset_config.image_height
            )
            
            # ä¿å­˜å›¾åƒ
            image_path = images_dir / f"image_{i:03d}.png"
            imageio.imwrite(image_path, (image * 255).astype(np.uint8))
            
            # ä¿å­˜poseçŸ©é˜µ
            pose_path = poses_dir / f"pose_{i:03d}.txt"
            np.savetxt(pose_path, transform_matrix)
            
            # æ·»åŠ åˆ°å˜æ¢æ•°æ®ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
            transforms["frames"].append({
                "file_path": f"images/image_{i:03d}.png",
                "transform_matrix": transform_matrix.tolist()
            })
        
        # ä¿å­˜å˜æ¢æ•°æ®
        with open(data_dir / "transforms.json", "w") as f:
            json.dump(transforms, f, indent=2)
            
        logger.info(f"ç”Ÿæˆäº† {n_views} ä¸ªåˆæˆè§†å›¾")
        
    def _generate_synthetic_image(
        self, camera_pos: np.ndarray, forward: np.ndarray, 
        width: int, height: int
    ) -> np.ndarray:
        """ç”Ÿæˆç®€å•çš„åˆæˆå›¾åƒ"""
        # åˆ›å»ºå…‰çº¿
        i, j = np.meshgrid(
            np.linspace(-1, 1, width),
            np.linspace(-1, 1, height),
            indexing='xy'
        )
        
        # è®¡ç®—å³å’Œä¸Šå‘é‡
        up = np.array([0.0, 0.0, 1.0])
        right = np.cross(forward, up)
        right = right / np.linalg.norm(right)
        up = np.cross(right, forward)
        
        # å…‰çº¿æ–¹å‘
        dirs = forward[None, None, :] + i[:, :, None] * right[None, None, :] + j[:, :, None] * up[None, None, :]
        dirs = dirs / np.linalg.norm(dirs, axis=2, keepdims=True)
        
        # ç®€å•çš„çƒä½“æ¸²æŸ“
        sphere_center = np.array([0.0, 0.0, 0.0])
        sphere_radius = 1.0
        
        # è®¡ç®—ä¸çƒä½“çš„äº¤ç‚¹
        oc = camera_pos - sphere_center
        a = np.sum(dirs * dirs, axis=2)
        b = 2.0 * np.sum(oc * dirs, axis=2)
        c = np.sum(oc * oc) - sphere_radius * sphere_radius
        
        discriminant = b * b - 4 * a * c
        
        # è®¡ç®—é¢œè‰²
        image = np.zeros((height, width, 3))
        
        # çƒä½“å†…éƒ¨
        hit_mask = discriminant >= 0
        if np.any(hit_mask):
            t = (-b - np.sqrt(discriminant)) / (2 * a)
            hit_points = camera_pos + t[:, :, None] * dirs
            
            # æ ¹æ®ä½ç½®è®¡ç®—é¢œè‰²
            colors = (hit_points + sphere_radius) / (2 * sphere_radius)
            colors = np.clip(colors, 0, 1)
            
            image[hit_mask] = colors[hit_mask]
        
        return image
        
    def setup_training(self):
        """è®¾ç½®è®­ç»ƒç»„ä»¶"""
        logger.info("è®¾ç½®è®­ç»ƒç»„ä»¶...")
        
        # åˆ›å»ºæ•°æ®é›†
        self.dataset = self._create_synthetic_dataset()
        
        # åˆ›å»ºæ¨¡å‹
        self.model = SVRasterModel(self.config).to(self.device)
        logger.info(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        
        # åˆ›å»ºè®­ç»ƒå™¨é…ç½®
        trainer_config = SVRasterTrainerConfig(
            learning_rate=1e-3,
            batch_size=1,  # SVRasterè®­ç»ƒä¸€èˆ¬ä½¿ç”¨batch_size=1
            num_epochs=50,  # æ¼”ç¤ºç”¨è¾ƒå°‘çš„epochs
            log_every=10,
            save_every=200,
            validate_every=100,
            checkpoint_dir="demos/checkpoints/svraster_training",
            use_amp=True,
            grad_clip_norm=1.0,
        )
        
        # åˆ›å»ºä½“ç§¯æ¸²æŸ“å™¨
        volume_renderer = VolumeRenderer(self.config)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        self.trainer = SVRasterTrainer(
            model=self.model,
            volume_renderer=volume_renderer,
            config=trainer_config,
            train_dataset=self.dataset,
            val_dataset=self.dataset  # æ¼”ç¤ºä¸­ä½¿ç”¨ç›¸åŒæ•°æ®é›†
        )
        
        logger.info("è®­ç»ƒç»„ä»¶è®¾ç½®å®Œæˆ")
        
    def run_training_epoch(self, epoch: int) -> Dict[str, float]:
        """è¿è¡Œä¸€ä¸ªè®­ç»ƒ epoch"""
        self.model.train()
        
        epoch_losses = {
            'total_loss': 0.0,
            'rgb_loss': 0.0,
            'depth_loss': 0.0,
            'regularization_loss': 0.0
        }
        
        num_batches = len(self.dataset) // self.trainer.config.batch_size
        
        with tqdm(range(num_batches), desc=f"Epoch {epoch}") as pbar:
            for batch_idx in pbar:
                # è·å–æ‰¹æ¬¡æ•°æ®
                batch_data = self._get_training_batch(batch_idx)
                
                # è®­ç»ƒæ­¥éª¤
                losses = self.trainer.train_step(batch_data)
                
                # ç´¯ç§¯æŸå¤±
                for key in epoch_losses:
                    if key in losses:
                        epoch_losses[key] += losses[key]
                
                # æ›´æ–°è¿›åº¦æ¡
                current_loss = losses.get('total_loss', 0.0)
                pbar.set_postfix({'loss': f'{current_loss:.6f}'})
        
        # å¹³å‡æŸå¤±
        for key in epoch_losses:
            epoch_losses[key] /= num_batches
            
        return epoch_losses
        
    def _get_training_batch(self, batch_idx: int) -> Dict[str, torch.Tensor]:
        """è·å–è®­ç»ƒæ‰¹æ¬¡æ•°æ®"""
        batch_size = self.trainer.config.batch_size
        
        # éšæœºé‡‡æ ·å…‰çº¿
        H, W = self.config.image_height, self.config.image_width
        
        # ç”Ÿæˆéšæœºåƒç´ åæ ‡
        pixels_y = torch.randint(0, H, (batch_size,), device=self.device)
        pixels_x = torch.randint(0, W, (batch_size,), device=self.device)
        
        # ç”Ÿæˆå…‰çº¿ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        camera_pos = torch.tensor([0.0, 0.0, 3.0], device=self.device)
        
        # å½’ä¸€åŒ–åƒç´ åæ ‡åˆ° [-1, 1]
        x_norm = (pixels_x.float() / W - 0.5) * 2
        y_norm = (pixels_y.float() / H - 0.5) * 2
        
        # å…‰çº¿æ–¹å‘
        ray_dirs = torch.stack([
            x_norm * 0.5,
            y_norm * 0.5,
            -torch.ones_like(x_norm)
        ], dim=1)
        ray_dirs = F.normalize(ray_dirs, dim=1)
        
        # å…‰çº¿èµ·ç‚¹
        ray_origins = camera_pos.unsqueeze(0).expand(batch_size, -1)
        
        # ç›®æ ‡é¢œè‰²ï¼ˆç®€åŒ–ï¼šåŸºäºå…‰çº¿æ–¹å‘çš„é¢œè‰²ï¼‰
        target_colors = (ray_dirs + 1) / 2  # å½’ä¸€åŒ–åˆ° [0, 1]
        
        return {
            'ray_origins': ray_origins,
            'ray_directions': ray_dirs,
            'target_rgb': target_colors,
            'pixels': torch.stack([pixels_x, pixels_y], dim=1)
        }
        
    def validate(self, epoch: int) -> Dict[str, float]:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        
        val_losses = {
            'val_rgb_loss': 0.0,
            'val_psnr': 0.0
        }
        
        num_val_batches = 10
        
        with torch.no_grad():
            for _ in range(num_val_batches):
                batch_data = self._get_training_batch(0)  # ç®€åŒ–ï¼šä½¿ç”¨ç›¸åŒçš„æ‰¹æ¬¡ç”Ÿæˆ
                
                # æ¸²æŸ“
                outputs = self.model(
                    batch_data['ray_origins'],
                    batch_data['ray_directions'],
                    mode="training"  # ä½¿ç”¨ä½“ç§¯æ¸²æŸ“
                )
                
                # è®¡ç®—æŸå¤±
                rgb_loss = F.mse_loss(outputs['rgb'], batch_data['target_rgb'])
                val_losses['val_rgb_loss'] += rgb_loss.item()
                
                # è®¡ç®— PSNR
                mse = rgb_loss.item()
                psnr = -10 * np.log10(mse + 1e-8)
                val_losses['val_psnr'] += psnr
        
        # å¹³å‡éªŒè¯æŒ‡æ ‡
        for key in val_losses:
            val_losses[key] /= num_val_batches
            
        return val_losses
        
    def run_full_training(self):
        """è¿è¡Œå®Œæ•´è®­ç»ƒ"""
        logger.info("å¼€å§‹ SVRaster è®­ç»ƒ...")
        
        best_psnr = 0.0
        training_history = []
        
        for epoch in range(self.trainer.config.num_epochs):
            start_time = time.time()
            
            # è®­ç»ƒ
            train_losses = self.run_training_epoch(epoch)
            
            # éªŒè¯
            val_metrics = {}
            if epoch % self.trainer.config.validate_every == 0:
                val_metrics = self.validate(epoch)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
                current_psnr = val_metrics.get('val_psnr', 0.0)
                if current_psnr > best_psnr:
                    best_psnr = current_psnr
                    self._save_checkpoint(epoch, "best_model.pth")
            
            epoch_time = time.time() - start_time
            
            # è®°å½•è®­ç»ƒå†å²
            epoch_info = {
                'epoch': epoch,
                'epoch_time': epoch_time,
                **train_losses,
                **val_metrics
            }
            training_history.append(epoch_info)
            
            # æ‰“å°è®­ç»ƒä¿¡æ¯
            logger.info(
                f"Epoch {epoch:3d} | "
                f"Loss: {train_losses['total_loss']:.6f} | "
                f"RGB: {train_losses['rgb_loss']:.6f} | "
                f"Time: {epoch_time:.1f}s"
            )
            
            if val_metrics:
                logger.info(
                    f"         | "
                    f"Val RGB: {val_metrics['val_rgb_loss']:.6f} | "
                    f"PSNR: {val_metrics['val_psnr']:.2f}dB"
                )
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % self.trainer.config.save_every == 0:
                self._save_checkpoint(epoch, f"checkpoint_epoch_{epoch}.pth")
        
        logger.info(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³ PSNR: {best_psnr:.2f}dB")
        
        # ä¿å­˜è®­ç»ƒå†å²
        self._save_training_history(training_history)
        
    def _save_checkpoint(self, epoch: int, filename: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = Path(self.trainer.config.checkpoint_dir)
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'config': self.config,
        }
        
        torch.save(checkpoint, checkpoint_dir / filename)
        logger.info(f"ä¿å­˜æ£€æŸ¥ç‚¹: {filename}")
        
    def _save_training_history(self, history: list):
        """ä¿å­˜è®­ç»ƒå†å²"""
        checkpoint_dir = Path(self.trainer.config.checkpoint_dir)
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        with open(checkpoint_dir / "training_history.json", "w") as f:
            json.dump(history, f, indent=2)
        
        logger.info("è®­ç»ƒå†å²å·²ä¿å­˜")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("SVRaster è®­ç»ƒæ¼”ç¤º")
    print("=" * 70)
    
    try:
        # åˆ›å»ºè®­ç»ƒæ¼”ç¤º
        demo = SVRasterTrainingDemo()
        
        # è®¾ç½®è®­ç»ƒ
        demo.setup_training()
        
        # è¿è¡Œè®­ç»ƒ
        demo.run_full_training()
        
        print("\nğŸ‰ SVRaster è®­ç»ƒæ¼”ç¤ºå®Œæˆï¼")
        print("\nè®­ç»ƒç‰¹ç‚¹:")
        print("âœ… ä½¿ç”¨ VolumeRenderer è¿›è¡Œä½“ç§¯æ¸²æŸ“è®­ç»ƒ")
        print("âœ… è‡ªé€‚åº”ç¨€ç–ä½“ç´ è¡¨ç¤º")
        print("âœ… çƒè°å‡½æ•°è§†è§’ç›¸å…³é¢œè‰²")
        print("âœ… ç°ä»£ PyTorch è®­ç»ƒå¾ªç¯")
        print("âœ… å®æ—¶æŸå¤±ç›‘æ§å’ŒéªŒè¯")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
