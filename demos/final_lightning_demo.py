from typing import Any
#!/usr/bin/env python3
"""
PyTorch Lightning NeRFé¡¹ç›®æœ€ç»ˆæ¼”ç¤º

è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†é¡¹ç›®ä¸­PyTorch Lightningçš„å®Œæ•´é›†æˆå’ŒåŠŸèƒ½
"""

import torch
import torch.nn.functional as F
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
from pytorch_lightning.loggers import TensorBoardLogger
import torchmetrics
from dataclasses import dataclass
import numpy as np
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

@dataclass
class NeRFLightningConfig:
    """PyTorch Lightning NeRFé…ç½®"""
    hidden_dim: int = 128
    num_layers: int = 4
    learning_rate: float = 5e-4
    pe_freq: int = 10  # ä½ç½®ç¼–ç é¢‘ç‡
    scheduler_type: str = "cosine"
    use_ema: bool = True
    ema_decay: float = 0.999

class NeRFLightningModule(pl.LightningModule):
    """PyTorch Lightning NeRFæ¨¡å—"""
    
    def __init__(self, config: NeRFLightningConfig):
        super().__init__()
        self.config = config
        self.save_hyperparameters()
        
        # æ„å»ºç½‘ç»œ
        layers = []
        input_dim = 3 * 2 * config.pe_freq + 3  # PEåçš„è¾“å…¥ç»´åº¦
        
        for i in range(config.num_layers):
            if i == 0:
                layers.append(torch.nn.Linear(input_dim, config.hidden_dim))
            else:
                layers.append(torch.nn.Linear(config.hidden_dim, config.hidden_dim))
            layers.append(torch.nn.ReLU())
        
        # è¾“å‡ºå±‚ï¼šå¯†åº¦å’Œé¢œè‰²
        layers.append(torch.nn.Linear(config.hidden_dim, 4))  # 1å¯†åº¦ + 3é¢œè‰²
        
        self.network = torch.nn.Sequential(*layers)
        
        # åˆå§‹åŒ–æŒ‡æ ‡
        self.train_psnr = torchmetrics.image.PeakSignalNoiseRatio()
        self.val_psnr = torchmetrics.image.PeakSignalNoiseRatio()
        self.val_ssim = torchmetrics.image.StructuralSimilarityIndexMeasure()
        
        # EMAæ¨¡å‹
        if config.use_ema:
            # åˆ›å»ºEMAæ¨¡å‹çš„å‰¯æœ¬
            self.ema_model = torch.nn.Sequential(*layers)
            self.ema_model.load_state_dict(self.network.state_dict())
            self.ema_model.eval()
            for param in self.ema_model.parameters():
                param.requires_grad_(False)
        else:
            self.ema_model = None
        
    def positional_encoding(self, x: torch.Tensor) -> torch.Tensor:
        """ä½ç½®ç¼–ç """
        encoded = []
        for freq in range(self.config.pe_freq):
            for func in [torch.sin, torch.cos]:
                encoded.append(func(x * (2.0 ** freq)))
        encoded.append(x)  # åŸå§‹åæ ‡
        return torch.cat(encoded, dim=-1)
    
    def forward(self, positions: torch.Tensor) -> dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        # ä½ç½®ç¼–ç 
        encoded_pos = self.positional_encoding(positions)
        
        # é€šè¿‡ç½‘ç»œ
        output = self.network(encoded_pos)
        
        # åˆ†ç¦»å¯†åº¦å’Œé¢œè‰²
        density = torch.relu(output[..., 0])  # å¯†åº¦ >= 0
        color = torch.sigmoid(output[..., 1:])  # é¢œè‰² [0, 1]
        
        return {
            'density': density, 'color': color
        }
    
    def training_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:
        """è®­ç»ƒæ­¥éª¤"""
        positions = batch['positions']  # [N, 3]
        target_colors = batch['colors']  # [N, 3]
        
        # å‰å‘ä¼ æ’­
        outputs = self(positions)
        
        # è®¡ç®—æŸå¤±
        color_loss = F.mse_loss(outputs['color'], target_colors)
        
        # è®¡ç®—PSNR
        psnr = self.train_psnr(outputs['color'], target_colors)
        
        # è®°å½•æŒ‡æ ‡
        self.log('train/loss', color_loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log('train/psnr', psnr, on_step=True, on_epoch=True, prog_bar=True)
        
        return color_loss
    
    def validation_step(
        self,
        batch: dict[str,
        torch.Tensor],
        batch_idx: int,
    )
        """éªŒè¯æ­¥éª¤"""
        positions = batch['positions']
        target_colors = batch['colors']
        
        # ä½¿ç”¨EMAæ¨¡å‹è¿›è¡ŒéªŒè¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.ema_model is not None:
            encoded_pos = self.positional_encoding(positions)
            output = self.ema_model(encoded_pos)
            density = torch.relu(output[..., 0])
            color = torch.sigmoid(output[..., 1:])
            outputs = {'density': density, 'color': color}
        else:
            outputs = self(positions)
        
        # è®¡ç®—æŸå¤±
        val_loss = F.mse_loss(outputs['color'], target_colors)
        
        # è®¡ç®—æŒ‡æ ‡
        psnr = self.val_psnr(outputs['color'], target_colors)
        ssim = self.val_ssim(
            outputs['color'].unsqueeze,
        )
        
        # è®°å½•éªŒè¯æŒ‡æ ‡
        self.log('val/loss', val_loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log('val/psnr', psnr, on_step=False, on_epoch=True, prog_bar=True)
        self.log('val/ssim', ssim, on_step=False, on_epoch=True, prog_bar=True)
        
        return {
            'val_loss': val_loss, 'val_psnr': psnr, 'val_ssim': ssim
        }
    
    def on_train_epoch_end(self):
        """è®­ç»ƒè½®æ¬¡ç»“æŸæ—¶æ›´æ–°EMA"""
        if self.ema_model is not None:
            self._update_ema()
    
    def _update_ema(self):
        """æ›´æ–°EMAæ¨¡å‹"""
        decay = self.config.ema_decay
        with torch.no_grad():
            for ema_param, model_param in zip(
                self.ema_model.parameters,
            )
                ema_param.mul_(decay).add_(model_param, alpha=1 - decay)
    
    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨"""
        optimizer = torch.optim.AdamW(
            self.parameters(), lr=self.config.learning_rate, weight_decay=1e-4
        )
        
        if self.config.scheduler_type == "cosine":
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer, T_max=100, eta_min=1e-6
            )
            return {
                "optimizer": optimizer, "lr_scheduler": {
                    "scheduler": scheduler, "interval": "epoch"
                }
            }
        else:
            return optimizer

class MockNeRFDataset(torch.utils.data.Dataset):
    """æ¨¡æ‹ŸNeRFæ•°æ®é›†"""
    
    def __init__(self, size: int = 1000):
        self.size = size
        
    def __len__(self):
        return self.size
    
    def __getitem__(self, idx):
        # ç”Ÿæˆéšæœº3Dä½ç½®å’Œå¯¹åº”çš„é¢œè‰²
        position = torch.randn(3) * 2  # [-2, 2] èŒƒå›´å†…çš„ä½ç½®
        
        # ç®€å•çš„é¢œè‰²å‡½æ•°ï¼šåŸºäºä½ç½®ç”Ÿæˆé¢œè‰²
        color = torch.sigmoid(position)  # å°†ä½ç½®æ˜ å°„åˆ°[0, 1]é¢œè‰²
        
        return {
            'positions': position, 'colors': color
        }

def demonstrate_complete_lightning():
    """æ¼”ç¤ºå®Œæ•´çš„PyTorch LightningåŠŸèƒ½"""
    
    print("ğŸŒŸ PyTorch Lightning NeRF å®Œæ•´åŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    
    # æ£€æŸ¥è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. åˆ›å»ºé…ç½®
    config = NeRFLightningConfig(
        hidden_dim=128, num_layers=4, learning_rate=1e-3, pe_freq=8, scheduler_type="cosine", use_ema=True, ema_decay=0.999
    )
    
    print(f"âš™ï¸  æ¨¡å‹é…ç½®: {config}")
    
    # 2. åˆ›å»ºæ¨¡å‹
    model = NeRFLightningModule(config)
    param_count = sum(p.numel() for p in model.parameters())
    print(f"ğŸ§  æ¨¡å‹å‚æ•°æ•°é‡: {param_count:, }")
    
    # 3. åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = MockNeRFDataset(1000)
    val_dataset = MockNeRFDataset(200)
    
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=64, shuffle=True, num_workers=0
    )
    val_loader = torch.utils.data.DataLoader(
        val_dataset, batch_size=64, shuffle=False, num_workers=0
    )
    
    print(f"ğŸ“Š è®­ç»ƒæ•°æ®: {len(train_dataset)} æ ·æœ¬")
    print(f"ğŸ“Š éªŒè¯æ•°æ®: {len(val_dataset)} æ ·æœ¬")
    
    # 4. åˆ›å»ºå›è°ƒå‡½æ•°
    callbacks = [
        # æ¨¡å‹æ£€æŸ¥ç‚¹
        ModelCheckpoint(
            dirpath="checkpoints/final_demo", filename="nerf-{
                epoch:02d,
            }
        ), # æ—©åœ
        EarlyStopping(
            monitor="val/psnr", mode="max", patience=20, min_delta=0.001
        ), # å­¦ä¹ ç‡ç›‘æ§
        LearningRateMonitor(
            logging_interval="epoch"
        )
    ]
    
    # 5. åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    logger = TensorBoardLogger(
        save_dir="logs", name="final_nerf_demo", version="v1.0"
    )
    
    # 6. åˆ›å»ºè®­ç»ƒå™¨
    trainer = pl.Trainer(
        max_epochs=50, devices=1, accelerator="auto", precision="16-mixed" if device == "cuda" else "32", callbacks=callbacks, logger=logger, log_every_n_steps=10, val_check_interval=0.5, # æ¯åŠä¸ªepochéªŒè¯ä¸€æ¬¡
        enable_checkpointing=True, enable_progress_bar=True, enable_model_summary=True
    )
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    print(f"ğŸ“… æœ€å¤§è½®æ¬¡: {trainer.max_epochs}")
    print(f"ğŸ¯ ç²¾åº¦: {trainer.precision}")
    print(f"ğŸ“ æ—¥å¿—ä¿å­˜åˆ°: logs/final_nerf_demo")
    print(f"ğŸ’¾ æ£€æŸ¥ç‚¹ä¿å­˜åˆ°: checkpoints/final_demo")
    
    # 7. å¼€å§‹è®­ç»ƒ
    trainer.fit(model, train_loader, val_loader)
    
    # 8. æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 60)
    print("âœ… è®­ç»ƒå®Œæˆ!")
    
    if trainer.callback_metrics:
        metrics = trainer.callback_metrics
        print(f"ğŸ“ˆ æœ€ç»ˆéªŒè¯PSNR: {metrics.get('val/psnr', 'N/A'):.3f}")
        print(f"ğŸ“ˆ æœ€ç»ˆéªŒè¯SSIM: {metrics.get('val/ssim', 'N/A'):.3f}")
        print(f"ğŸ“‰ æœ€ç»ˆéªŒè¯æŸå¤±: {metrics.get('val/loss', 'N/A'):.6f}")
    
    # 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = "checkpoints/final_demo/final_model.ckpt"
    trainer.save_checkpoint(final_model_path)
    print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹ä¿å­˜åˆ°: {final_model_path}")
    
    print("\nğŸ‰ PyTorch Lightningæ¼”ç¤ºå®Œæˆ!")
    print("\nğŸ“‹ æ€»ç»“:")
    print("  âœ… è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ")
    print("  âœ… EMAæ¨¡å‹æ›´æ–°")
    print("  âœ… è‡ªåŠ¨æ£€æŸ¥ç‚¹ä¿å­˜")
    print("  âœ… æ—©åœæœºåˆ¶")
    print("  âœ… å­¦ä¹ ç‡è°ƒåº¦")
    print("  âœ… TensorBoardæ—¥å¿—è®°å½•")
    print("  âœ… å¤šç§è¯„ä¼°æŒ‡æ ‡")
    print("  âœ… è¿›åº¦æ¡å’Œæ¨¡å‹æ‘˜è¦")
    
    print(f"\nğŸ“Š æŸ¥çœ‹è®­ç»ƒæ—¥å¿—: tensorboard --logdir logs")
    
    return model, trainer

if __name__ == '__main__':
    print("å¯åŠ¨PyTorch Lightning NeRFå®Œæ•´æ¼”ç¤º...")
    model, trainer = demonstrate_complete_lightning()
    print("æ¼”ç¤ºå®Œæˆ!") 