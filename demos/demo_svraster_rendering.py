#!/usr/bin/env python3
"""
SVRaster é«˜æ•ˆæ¸²æŸ“æ¼”ç¤º

è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºå¦‚ä½•ä½¿ç”¨ SVRaster è¿›è¡Œå®æ—¶é«˜æ•ˆæ¸²æŸ“ã€‚
é‡ç‚¹å±•ç¤ºæ¨ç†é˜¶æ®µçš„å…‰æ …åŒ–æ¸²æŸ“æ€§èƒ½å’Œè´¨é‡ã€‚

ç‰¹ç‚¹ï¼š
- ä½¿ç”¨ VoxelRasterizer è¿›è¡Œå¿«é€Ÿå…‰æ …åŒ–
- å®æ—¶æ¸²æŸ“æ€§èƒ½ä¼˜åŒ–
- å¤šç§æ¸²æŸ“æ¨¡å¼å¯¹æ¯”
- GPU åŠ é€Ÿæ¸²æŸ“
- æ¸²æŸ“è´¨é‡è¯„ä¼°
"""

from __future__ import annotations

import sys
import torch
import numpy as np
import time
import logging
from pathlib import Path
from typing import Dict, Tuple, Optional, List
import json
import imageio
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'src'))

# SVRaster å¯¼å…¥
from src.nerfs.svraster import (
    SVRasterConfig, SVRasterModel,
    SVRasterRenderer, SVRasterRendererConfig,
    VoxelRasterizer, VolumeRenderer
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SVRasterRenderingDemo:
    """SVRaster é«˜æ•ˆæ¸²æŸ“æ¼”ç¤ºç±»"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æ¸²æŸ“é…ç½®
        self.model_config = self._create_model_config()
        self.render_config = self._create_render_config()
        
        self.model = None
        self.volume_renderer = None
        self.true_rasterizer = None
        self.svraster_renderer = None
        
    def _create_model_config(self) -> SVRasterConfig:
        """åˆ›å»ºæ¨¡å‹é…ç½®"""
        config = SVRasterConfig(
            # åœºæ™¯è®¾ç½®
            image_width=800,
            image_height=600,
            scene_bounds=(-2.0, -2.0, -2.0, 2.0, 2.0, 2.0),
            
            # ä½“ç´ ç½‘æ ¼è®¾ç½®
            base_resolution=128,  # é«˜åˆ†è¾¨ç‡ç”¨äºé«˜è´¨é‡æ¸²æŸ“
            max_octree_levels=10,
            
            # æ¸²æŸ“è®¾ç½®
            ray_samples_per_voxel=4,  # æ¨ç†æ—¶å¯ä»¥å‡å°‘é‡‡æ ·
            depth_peeling_layers=2,
            morton_ordering=True,
            
            # å¤–è§‚è®¾ç½®
            sh_degree=2,
            color_activation="sigmoid",
            density_activation="exp",
            
            # æ¸²æŸ“è®¾ç½®
            background_color=(1.0, 1.0, 1.0),  # ç™½è‰²èƒŒæ™¯
            near_plane=0.1,
            far_plane=10.0,
        )
        
        logger.info("æ¨¡å‹é…ç½®åˆ›å»ºå®Œæˆ")
        logger.info(f"  - æ¸²æŸ“åˆ†è¾¨ç‡: {config.image_width}x{config.image_height}")
        logger.info(f"  - ä½“ç´ åˆ†è¾¨ç‡: {config.base_resolution}^3")
        
        return config
        
    def _create_render_config(self) -> SVRasterRendererConfig:
        """åˆ›å»ºæ¸²æŸ“å™¨é…ç½®"""
        config = SVRasterRendererConfig(
            image_width=self.model_config.image_width,
            image_height=self.model_config.image_height,
            render_batch_size=8192,  # å¤§æ‰¹æ¬¡æé«˜æ•ˆç‡
            render_chunk_size=2048,
            background_color=(1.0, 1.0, 1.0),
            use_alpha_blending=True,
            depth_threshold=1e-6,
            max_rays_per_batch=16384,
            use_hierarchical_sampling=True,
            output_format='png',
            save_depth=True,
            save_alpha=True,
            use_cached_features=True,
            enable_gradient_checkpointing=False,  # æ¨ç†æ—¶å…³é—­
        )
        
        logger.info("æ¸²æŸ“å™¨é…ç½®åˆ›å»ºå®Œæˆ")
        logger.info(f"  - æ‰¹æ¬¡å¤§å°: {config.render_batch_size}")
        logger.info(f"  - æœ€å¤§å…‰çº¿æ•°: {config.max_rays_per_batch}")
        
        return config
        
    def setup_model_and_renderers(self):
        """è®¾ç½®æ¨¡å‹å’Œæ¸²æŸ“å™¨"""
        logger.info("è®¾ç½®æ¨¡å‹å’Œæ¸²æŸ“å™¨...")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = SVRasterModel(self.model_config).to(self.device)
        
        # åˆå§‹åŒ–æ¨¡å‹ä¸ºæ¼”ç¤ºåœºæ™¯
        self._initialize_demo_scene()
        
        # åˆ›å»ºä½“ç§¯æ¸²æŸ“å™¨ï¼ˆè®­ç»ƒç”¨ï¼‰
        self.volume_renderer = VolumeRenderer(self.model_config)
        
        # åˆ›å»ºçœŸæ­£çš„å…‰æ …åŒ–å™¨ï¼ˆæ¨ç†ç”¨ï¼‰
        self.true_rasterizer = VoxelRasterizer(self.model_config)
        
        # åˆ›å»º SVRaster æ¸²æŸ“å™¨ï¼ˆé«˜çº§æ¥å£ï¼‰
        self.svraster_renderer = SVRasterRenderer(
            model=self.model,
            rasterizer=self.true_rasterizer,
            config=self.render_config
        )
        
        logger.info("æ¨¡å‹å’Œæ¸²æŸ“å™¨è®¾ç½®å®Œæˆ")
        logger.info(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        
    def _initialize_demo_scene(self):
        """åˆå§‹åŒ–æ¼”ç¤ºåœºæ™¯"""
        logger.info("åˆå§‹åŒ–æ¼”ç¤ºåœºæ™¯...")
        
        with torch.no_grad():
            # åˆ›å»ºå¤šä¸ªçƒä½“çš„å¤æ‚åœºæ™¯
            res = self.model_config.base_resolution
            
            # ç”Ÿæˆç½‘æ ¼åæ ‡
            coords = torch.stack(torch.meshgrid(
                torch.linspace(-2, 2, res),
                torch.linspace(-2, 2, res),
                torch.linspace(-2, 2, res),
                indexing='ij'
            ), dim=-1).to(self.device)
            
            # åˆ›å»ºå¤šä¸ªçƒä½“
            densities = torch.zeros(res, res, res, device=self.device)
            
            # çƒä½“ 1: ä¸­å¿ƒçƒï¼ˆçº¢è‰²ï¼‰
            center1 = torch.tensor([0.0, 0.0, 0.0], device=self.device)
            dist1 = torch.norm(coords - center1, dim=-1)
            sphere1 = torch.exp(-(dist1 - 0.8) ** 2 / 0.1)
            densities += sphere1
            
            # çƒä½“ 2: å·¦ä¾§çƒï¼ˆç»¿è‰²ï¼‰
            center2 = torch.tensor([-1.2, 0.0, 0.0], device=self.device)
            dist2 = torch.norm(coords - center2, dim=-1)
            sphere2 = torch.exp(-(dist2 - 0.5) ** 2 / 0.08)
            densities += sphere2 * 0.8
            
            # çƒä½“ 3: å³ä¾§çƒï¼ˆè“è‰²ï¼‰
            center3 = torch.tensor([1.2, 0.0, 0.0], device=self.device)
            dist3 = torch.norm(coords - center3, dim=-1)
            sphere3 = torch.exp(-(dist3 - 0.5) ** 2 / 0.08)
            densities += sphere3 * 0.6
            
            # è®¾ç½®å¯†åº¦
            self.model.voxels.densities = densities.unsqueeze(-1)
            
            # åˆ›å»ºå½©è‰²ç‰¹å¾ï¼ˆçƒè°ç³»æ•°ï¼‰
            num_sh_coeffs = (self.model_config.sh_degree + 1) ** 2
            features = torch.zeros(res, res, res, 3 * num_sh_coeffs, device=self.device)
            
            # ä¸ºä¸åŒåŒºåŸŸè®¾ç½®ä¸åŒé¢œè‰²ï¼Œä½¿ç”¨æ­£ç¡®çš„ç´¢å¼•æ–¹æ³•
            # ä¸­å¿ƒçƒï¼šçº¢è‰²
            mask1 = sphere1 > 0.1
            i1, j1, k1 = torch.where(mask1)
            features[i1, j1, k1, 0] = 0.8  # R é€šé“çš„ 0 é˜¶ç³»æ•°
            features[i1, j1, k1, num_sh_coeffs] = 0.2  # G é€šé“çš„ 0 é˜¶ç³»æ•°
            features[i1, j1, k1, 2 * num_sh_coeffs] = 0.2  # B é€šé“çš„ 0 é˜¶ç³»æ•°
            
            # å·¦ä¾§çƒï¼šç»¿è‰²
            mask2 = sphere2 > 0.1
            i2, j2, k2 = torch.where(mask2)
            features[i2, j2, k2, 0] = 0.2
            features[i2, j2, k2, num_sh_coeffs] = 0.8
            features[i2, j2, k2, 2 * num_sh_coeffs] = 0.2
            
            # å³ä¾§çƒï¼šè“è‰²
            mask3 = sphere3 > 0.1
            i3, j3, k3 = torch.where(mask3)
            features[i3, j3, k3, 0] = 0.2
            features[i3, j3, k3, num_sh_coeffs] = 0.2
            features[i3, j3, k3, 2 * num_sh_coeffs] = 0.8
            
            # æ·»åŠ ä¸€äº›è§†è§’ç›¸å…³æ•ˆæœï¼ˆé«˜é˜¶çƒè°ï¼‰
            if num_sh_coeffs > 1:
                # ä¸ºæ¯ä¸ªçƒä½“æ·»åŠ è½»å¾®çš„è§†è§’ä¾èµ–
                features[i1, j1, k1, 1:4] = 0.1  # 1é˜¶ç³»æ•°
                features[i2, j2, k2, num_sh_coeffs+1:num_sh_coeffs+4] = 0.1
                features[i3, j3, k3, 2*num_sh_coeffs+1:2*num_sh_coeffs+4] = 0.1
            
            self.model.voxels.colors = features
            
        logger.info("æ¼”ç¤ºåœºæ™¯åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  - å¯†åº¦èŒƒå›´: [{densities.min():.3f}, {densities.max():.3f}]")
        logger.info(f"  - ç‰¹å¾ç»´åº¦: {features.shape[-1]}")
        
    def generate_camera_path(self, num_frames: int = 60) -> List[Tuple[np.ndarray, np.ndarray]]:
        """ç”Ÿæˆç›¸æœºè·¯å¾„"""
        camera_path = []
        
        # åœ†å½¢è½¨é“
        radius = 4.0
        for i in range(num_frames):
            angle = 2 * np.pi * i / num_frames
            
            # ç›¸æœºä½ç½®
            camera_pos = np.array([
                radius * np.cos(angle),
                radius * np.sin(angle),
                1.0  # ç¨å¾®ä»ä¸Šå¾€ä¸‹çœ‹
            ])
            
            # æœå‘åœºæ™¯ä¸­å¿ƒ
            target = np.array([0.0, 0.0, 0.0])
            forward = target - camera_pos
            forward = forward / np.linalg.norm(forward)
            
            camera_path.append((camera_pos, forward))
            
        return camera_path
        
    def generate_ray_batch(
        self, camera_pos: np.ndarray, camera_forward: np.ndarray, 
        subset_ratio: float = 1.0
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """ä¸ºç»™å®šç›¸æœºä½ç½®ç”Ÿæˆå…‰çº¿"""
        H, W = self.model_config.image_height, self.model_config.image_width
        
        # ç”Ÿæˆåƒç´ ç½‘æ ¼
        if subset_ratio < 1.0:
            # å­é›†æ¸²æŸ“ï¼ˆç”¨äºæ€§èƒ½æµ‹è¯•ï¼‰
            subset_H = int(H * subset_ratio)
            subset_W = int(W * subset_ratio)
            i, j = torch.meshgrid(
                torch.linspace(0, W-1, subset_W),
                torch.linspace(0, H-1, subset_H),
                indexing='xy'
            )
            pixels = torch.stack([i, j], dim=-1).reshape(-1, 2)
        else:
            # å®Œæ•´æ¸²æŸ“
            i, j = torch.meshgrid(
                torch.arange(W),
                torch.arange(H),
                indexing='xy'
            )
            pixels = torch.stack([i, j], dim=-1).reshape(-1, 2)
        
        # å½’ä¸€åŒ–åˆ° [-1, 1]
        x_norm = (pixels[:, 0] / W - 0.5) * 2
        y_norm = (pixels[:, 1] / H - 0.5) * 2
        
        # è®¡ç®—ç›¸æœºåæ ‡ç³»
        up = np.array([0.0, 0.0, 1.0])
        right = np.cross(camera_forward, up)
        right = right / np.linalg.norm(right)
        up = np.cross(right, camera_forward)
        
        # è½¬æ¢ä¸ºtensor
        camera_forward_t = torch.tensor(camera_forward, dtype=torch.float32)
        right_t = torch.tensor(right, dtype=torch.float32)
        up_t = torch.tensor(up, dtype=torch.float32)
        
        # ç„¦è·ï¼ˆfield of viewï¼‰
        focal_length = 1.0
        
        # å…‰çº¿æ–¹å‘
        ray_dirs = (
            camera_forward_t[None, :] * focal_length +
            right_t[None, :] * x_norm[:, None] * 0.5 +
            up_t[None, :] * y_norm[:, None] * 0.5
        )
        ray_dirs = ray_dirs / torch.norm(ray_dirs, dim=1, keepdim=True)
        
        # è½¬æ¢ä¸º tensor
        ray_origins = torch.tensor(camera_pos, dtype=torch.float32, device=self.device)
        ray_origins = ray_origins.unsqueeze(0).expand(len(pixels), -1)
        ray_directions = torch.tensor(ray_dirs, dtype=torch.float32, device=self.device)
        
        return ray_origins, ray_directions
        
    def benchmark_rendering_modes(self):
        """å¯¹æ¯”ä¸åŒæ¸²æŸ“æ¨¡å¼çš„æ€§èƒ½"""
        logger.info("å¼€å§‹æ¸²æŸ“æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        # ç”Ÿæˆæµ‹è¯•ç›¸æœºä½ç½®
        camera_pos = np.array([3.0, 0.0, 1.0])
        target = np.array([0.0, 0.0, 0.0])
        camera_forward = target - camera_pos
        camera_forward = camera_forward / np.linalg.norm(camera_forward)
        
        # æµ‹è¯•ä¸åŒçš„æ¸²æŸ“åˆ†è¾¨ç‡
        test_ratios = [0.25, 0.5, 1.0]  # 1/4, 1/2, å…¨åˆ†è¾¨ç‡
        
        results = {}
        
        for ratio in test_ratios:
            logger.info(f"\næµ‹è¯•åˆ†è¾¨ç‡æ¯”ä¾‹: {ratio}")
            
            # ç”Ÿæˆå…‰çº¿
            ray_origins, ray_directions = self.generate_ray_batch(
                camera_pos, camera_forward, ratio
            )
            
            num_rays = len(ray_origins)
            actual_res = f"{int(self.model_config.image_width * ratio)}x{int(self.model_config.image_height * ratio)}"
            logger.info(f"  å…‰çº¿æ•°é‡: {num_rays:,} ({actual_res})")
            
            # 1. ä½“ç§¯æ¸²æŸ“ï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰
            logger.info("  æµ‹è¯•ä½“ç§¯æ¸²æŸ“...")
            with torch.no_grad():
                torch.cuda.synchronize() if torch.cuda.is_available() else None
                start_time = time.time()
                
                volume_outputs = self.model(
                    ray_origins, ray_directions, mode="training"
                )
                
                torch.cuda.synchronize() if torch.cuda.is_available() else None
                volume_time = time.time() - start_time
            
            # 2. å…‰æ …åŒ–æ¸²æŸ“ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
            logger.info("  æµ‹è¯•å…‰æ …åŒ–æ¸²æŸ“...")
            with torch.no_grad():
                torch.cuda.synchronize() if torch.cuda.is_available() else None
                start_time = time.time()
                
                raster_outputs = self.model(
                    ray_origins, ray_directions, mode="inference"
                )
                
                torch.cuda.synchronize() if torch.cuda.is_available() else None
                raster_time = time.time() - start_time
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            volume_fps = 1.0 / volume_time if volume_time > 0 else float('inf')
            raster_fps = 1.0 / raster_time if raster_time > 0 else float('inf')
            speedup = volume_time / raster_time if raster_time > 0 else float('inf')
            
            results[ratio] = {
                'resolution': actual_res,
                'num_rays': num_rays,
                'volume_time': volume_time,
                'raster_time': raster_time,
                'volume_fps': volume_fps,
                'raster_fps': raster_fps,
                'speedup': speedup,
                'volume_outputs': volume_outputs,
                'raster_outputs': raster_outputs
            }
            
            logger.info(f"  ä½“ç§¯æ¸²æŸ“: {volume_time:.4f}s ({volume_fps:.1f} FPS)")
            logger.info(f"  å…‰æ …åŒ–æ¸²æŸ“: {raster_time:.4f}s ({raster_fps:.1f} FPS)")
            logger.info(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")
            
        return results
        
    def render_quality_comparison(self, camera_pos: np.ndarray, camera_forward: np.ndarray):
        """æ¸²æŸ“è´¨é‡å¯¹æ¯”"""
        logger.info("ç”Ÿæˆè´¨é‡å¯¹æ¯”å›¾åƒ...")
        
        # ç”Ÿæˆå…‰çº¿
        ray_origins, ray_directions = self.generate_ray_batch(camera_pos, camera_forward)
        
        with torch.no_grad():
            # ä½“ç§¯æ¸²æŸ“
            volume_outputs = self.model(ray_origins, ray_directions, mode="training")
            
            # å…‰æ …åŒ–æ¸²æŸ“
            raster_outputs = self.model(ray_origins, ray_directions, mode="inference")
        
        # é‡æ–°æ•´ç†ä¸ºå›¾åƒæ ¼å¼
        H, W = self.model_config.image_height, self.model_config.image_width
        
        volume_image = volume_outputs['rgb'].reshape(H, W, 3).cpu().numpy()
        raster_image = raster_outputs['rgb'].reshape(H, W, 3).cpu().numpy()
        
        # æ·±åº¦å›¾
        volume_depth = volume_outputs['depth'].reshape(H, W).cpu().numpy()
        raster_depth = raster_outputs['depth'].reshape(H, W).cpu().numpy()
        
        # å½’ä¸€åŒ–æ·±åº¦å›¾
        volume_depth_norm = (volume_depth - volume_depth.min()) / (volume_depth.max() - volume_depth.min() + 1e-8)
        raster_depth_norm = (raster_depth - raster_depth.min()) / (raster_depth.max() - raster_depth.min() + 1e-8)
        
        # è®¡ç®—å·®å¼‚å›¾
        rgb_diff = np.abs(volume_image - raster_image)
        depth_diff = np.abs(volume_depth_norm - raster_depth_norm)
        
        return {
            'volume_rgb': volume_image,
            'raster_rgb': raster_image,
            'volume_depth': volume_depth_norm,
            'raster_depth': raster_depth_norm,
            'rgb_diff': rgb_diff,
            'depth_diff': depth_diff
        }
        
    def save_comparison_images(self, comparison_data: dict, output_dir: Path):
        """ä¿å­˜å¯¹æ¯”å›¾åƒ"""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ RGB å›¾åƒ
        imageio.imwrite(
            output_dir / "volume_rendering.png",
            (np.clip(comparison_data['volume_rgb'], 0, 1) * 255).astype(np.uint8)
        )
        
        imageio.imwrite(
            output_dir / "raster_rendering.png",
            (np.clip(comparison_data['raster_rgb'], 0, 1) * 255).astype(np.uint8)
        )
        
        # ä¿å­˜æ·±åº¦å›¾
        imageio.imwrite(
            output_dir / "volume_depth.png",
            (comparison_data['volume_depth'] * 255).astype(np.uint8)
        )
        
        imageio.imwrite(
            output_dir / "raster_depth.png",
            (comparison_data['raster_depth'] * 255).astype(np.uint8)
        )
        
        # ä¿å­˜å·®å¼‚å›¾
        imageio.imwrite(
            output_dir / "rgb_difference.png",
            (np.clip(comparison_data['rgb_diff'] * 10, 0, 1) * 255).astype(np.uint8)  # æ”¾å¤§å·®å¼‚
        )
        
        imageio.imwrite(
            output_dir / "depth_difference.png",
            (np.clip(comparison_data['depth_diff'] * 10, 0, 1) * 255).astype(np.uint8)
        )
        
        logger.info(f"å¯¹æ¯”å›¾åƒå·²ä¿å­˜åˆ°: {output_dir}")
        
    def render_animation(self, output_dir: Path, num_frames: int = 30):
        """æ¸²æŸ“åŠ¨ç”»åºåˆ—"""
        logger.info(f"æ¸²æŸ“ {num_frames} å¸§åŠ¨ç”»...")
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆç›¸æœºè·¯å¾„
        camera_path = self.generate_camera_path(num_frames)
        
        render_times = []
        
        with tqdm(enumerate(camera_path), total=num_frames, desc="æ¸²æŸ“åŠ¨ç”»") as pbar:
            for frame_idx, (camera_pos, camera_forward) in pbar:
                # ç”Ÿæˆå…‰çº¿
                ray_origins, ray_directions = self.generate_ray_batch(
                    camera_pos, camera_forward, subset_ratio=0.5  # ä½¿ç”¨ä¸€åŠåˆ†è¾¨ç‡æé«˜é€Ÿåº¦
                )
                
                # æ¸²æŸ“
                start_time = time.time()
                with torch.no_grad():
                    outputs = self.model(ray_origins, ray_directions, mode="inference")
                render_time = time.time() - start_time
                render_times.append(render_time)
                
                # è½¬æ¢ä¸ºå›¾åƒ
                H, W = int(self.model_config.image_height * 0.5), int(self.model_config.image_width * 0.5)
                image = outputs['rgb'].reshape(H, W, 3).cpu().numpy()
                image = np.clip(image, 0, 1)
                
                # ä¿å­˜å¸§
                frame_path = output_dir / f"frame_{frame_idx:03d}.png"
                imageio.imwrite(frame_path, (image * 255).astype(np.uint8))
                
                # æ›´æ–°è¿›åº¦æ¡
                avg_render_time = np.mean(render_times)
                fps = 1.0 / avg_render_time if avg_render_time > 0 else 0
                pbar.set_postfix({
                    'avg_time': f'{avg_render_time:.3f}s',
                    'fps': f'{fps:.1f}'
                })
        
        avg_fps = 1.0 / np.mean(render_times)
        logger.info(f"åŠ¨ç”»æ¸²æŸ“å®Œæˆï¼å¹³å‡å¸§ç‡: {avg_fps:.1f} FPS")
        
        return render_times
        
    def run_full_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        logger.info("å¼€å§‹ SVRaster é«˜æ•ˆæ¸²æŸ“æ¼”ç¤º...")
        
        # æ€§èƒ½åŸºå‡†æµ‹è¯•
        benchmark_results = self.benchmark_rendering_modes()
        
        # è´¨é‡å¯¹æ¯”
        camera_pos = np.array([3.0, 1.0, 1.5])
        target = np.array([0.0, 0.0, 0.0])
        camera_forward = target - camera_pos
        camera_forward = camera_forward / np.linalg.norm(camera_forward)
        
        comparison_data = self.render_quality_comparison(camera_pos, camera_forward)
        
        # ä¿å­˜å¯¹æ¯”å›¾åƒ
        output_dir = Path("demos/demo_outputs/svraster_rendering")
        self.save_comparison_images(comparison_data, output_dir)
        
        # æ¸²æŸ“åŠ¨ç”»
        animation_times = self.render_animation(
            output_dir / "animation", num_frames=24
        )
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_performance_report(
            benchmark_results, comparison_data, animation_times, output_dir
        )
        
    def _generate_performance_report(
        self, benchmark_results: dict, comparison_data: dict, 
        animation_times: list, output_dir: Path
    ):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = {
            "device": str(self.device),
            "model_config": {
                "resolution": f"{self.model_config.image_width}x{self.model_config.image_height}",
                "base_resolution": self.model_config.base_resolution,
                "sh_degree": self.model_config.sh_degree
            },
            "benchmark_results": {},
            "animation_performance": {
                "num_frames": len(animation_times),
                "avg_frame_time": float(np.mean(animation_times)),
                "avg_fps": float(1.0 / np.mean(animation_times)),
                "min_frame_time": float(np.min(animation_times)),
                "max_frame_time": float(np.max(animation_times))
            },
            "quality_metrics": {
                "rgb_mse": float(np.mean(comparison_data['rgb_diff'] ** 2)),
                "depth_mse": float(np.mean(comparison_data['depth_diff'] ** 2)),
                "rgb_psnr": float(-10 * np.log10(np.mean(comparison_data['rgb_diff'] ** 2) + 1e-8))
            }
        }
        
        # æ·»åŠ åŸºå‡†æµ‹è¯•ç»“æœ
        for ratio, results in benchmark_results.items():
            report["benchmark_results"][f"resolution_{ratio}"] = {
                "resolution": results['resolution'],
                "num_rays": results['num_rays'],
                "volume_time": results['volume_time'],
                "raster_time": results['raster_time'],
                "volume_fps": results['volume_fps'],
                "raster_fps": results['raster_fps'],
                "speedup": results['speedup']
            }
        
        # ä¿å­˜æŠ¥å‘Š
        with open(output_dir / "performance_report.json", "w") as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_dir / 'performance_report.json'}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("SVRaster é«˜æ•ˆæ¸²æŸ“æ¼”ç¤º")
    print("=" * 70)
    
    try:
        # åˆ›å»ºæ¸²æŸ“æ¼”ç¤º
        demo = SVRasterRenderingDemo()
        
        # è®¾ç½®æ¨¡å‹å’Œæ¸²æŸ“å™¨
        demo.setup_model_and_renderers()
        
        # è¿è¡Œå®Œæ•´æ¼”ç¤º
        demo.run_full_demo()
        
        print("\nğŸ‰ SVRaster é«˜æ•ˆæ¸²æŸ“æ¼”ç¤ºå®Œæˆï¼")
        print("\næ¸²æŸ“ç‰¹ç‚¹:")
        print("âœ… ä½¿ç”¨ VoxelRasterizer è¿›è¡Œå®æ—¶å…‰æ …åŒ–")
        print("âœ… GPU åŠ é€Ÿé«˜æ€§èƒ½æ¸²æŸ“")
        print("âœ… å¤šåˆ†è¾¨ç‡æ€§èƒ½æµ‹è¯•")
        print("âœ… è®­ç»ƒ/æ¨ç†æ¨¡å¼è´¨é‡å¯¹æ¯”")
        print("âœ… å®æ—¶åŠ¨ç”»åºåˆ—æ¸²æŸ“")
        print("\nè¾“å‡ºæ–‡ä»¶:")
        print("ğŸ“ demos/demo_outputs/svraster_rendering/")
        print("   â”œâ”€â”€ volume_rendering.png      # ä½“ç§¯æ¸²æŸ“ç»“æœ")
        print("   â”œâ”€â”€ raster_rendering.png      # å…‰æ …åŒ–æ¸²æŸ“ç»“æœ")
        print("   â”œâ”€â”€ *_depth.png               # æ·±åº¦å›¾")
        print("   â”œâ”€â”€ *_difference.png          # å·®å¼‚å›¾")
        print("   â”œâ”€â”€ animation/                # åŠ¨ç”»åºåˆ—")
        print("   â””â”€â”€ performance_report.json   # æ€§èƒ½æŠ¥å‘Š")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æ¸²æŸ“è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¸²æŸ“å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
