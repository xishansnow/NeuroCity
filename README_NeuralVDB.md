# NeuralVDB: Efficient Sparse Volumetric Neural Representations

åŸºäºè®ºæ–‡ "NeuralVDB: Efficient Sparse Volumetric Neural Representations" çš„å®Œæ•´å®ç°ï¼Œæä¾›äº†é«˜æ•ˆçš„ç¨€ç–ä½“ç´ ç¥ç»è¡¨ç¤ºè§£å†³æ–¹æ¡ˆã€‚

## ğŸš€ ä¸»è¦ç‰¹æ€§

### åŸºç¡€ç‰ˆæœ¬ (`neural_vdb.py`)
- **ç¨€ç–ä½“ç´ è¡¨ç¤º**: åŸºäºå…«å‰æ ‘çš„è‡ªé€‚åº”ç¨€ç–ä½“ç´ ç½‘æ ¼
- **ç¥ç»ç½‘ç»œç¼–ç **: å°†3Dåæ ‡æ˜ å°„åˆ°ç‰¹å¾å‘é‡ï¼Œå†é¢„æµ‹å ç”¨å€¼
- **å†…å­˜ä¼˜åŒ–**: ç›¸æ¯”ä¼ ç»Ÿä½“ç´ åŒ–æ–¹æ³•æ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨
- **é«˜æ•ˆè®­ç»ƒ**: æ”¯æŒæ‰¹é‡è®­ç»ƒå’Œæ—©åœæœºåˆ¶
- **å¯è§†åŒ–**: å…«å‰æ ‘ç»“æ„å’Œé¢„æµ‹ç»“æœçš„å¯è§†åŒ–

### é«˜çº§ç‰ˆæœ¬ (`neural_vdb_advanced.py`)
- **è‡ªé€‚åº”åˆ†è¾¨ç‡**: æ ¹æ®æ•°æ®å¯†åº¦è‡ªåŠ¨è°ƒæ•´åˆ†è¾¨ç‡
- **å¤šå°ºåº¦ç‰¹å¾æå–**: æ•è·ä¸åŒå°ºåº¦çš„å‡ ä½•ç‰¹å¾
- **é«˜çº§æŸå¤±å‡½æ•°**: åŒ…å«å¹³æ»‘æ€§ã€ç¨€ç–æ€§ã€ä¸€è‡´æ€§æŸå¤±
- **æ¸è¿›å¼è®­ç»ƒ**: åŠ¨æ€è°ƒæ•´è®­ç»ƒå‚æ•°
- **ç‰¹å¾å‹ç¼©**: æ”¯æŒç‰¹å¾é‡åŒ–å’Œå‹ç¼©
- **åŠ¨æ€å…«å‰æ ‘ä¼˜åŒ–**: åŸºäºé‡è¦æ€§çš„èŠ‚ç‚¹ç»†åˆ†

## ğŸ“ æ–‡ä»¶ç»“æ„

```
NeuroCity/
â”œâ”€â”€ neural_vdb.py              # åŸºç¡€NeuralVDBå®ç°
â”œâ”€â”€ neural_vdb_advanced.py     # é«˜çº§NeuralVDBå®ç°
â”œâ”€â”€ neural_vdb_example.py      # ä½¿ç”¨ç¤ºä¾‹å’Œæ¼”ç¤º
â”œâ”€â”€ README_NeuralVDB.md        # æœ¬æ–‡æ¡£
â””â”€â”€ requirements.txt           # ä¾èµ–åŒ…åˆ—è¡¨
```

## ğŸ› ï¸ å®‰è£…ä¾èµ–

```bash
# å®‰è£…åŸºç¡€ä¾èµ–
pip install torch numpy matplotlib tqdm scipy scikit-learn

# æˆ–è€…ä½¿ç”¨é¡¹ç›®æä¾›çš„å®‰è£…è„šæœ¬
chmod +x install_dependencies.sh
./install_dependencies.sh
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€NeuralVDBä½¿ç”¨

```python
from neural_vdb import NeuralVDB, NeuralVDBConfig, create_sample_data

# åˆ›å»ºé…ç½®
config = NeuralVDBConfig(
    voxel_size=1.0,
    max_depth=6,
    feature_dim=32,
    learning_rate=1e-3,
    batch_size=1024
)

# åˆ›å»ºç¤ºä¾‹æ•°æ®
points, occupancies = create_sample_data(5000)

# åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
neural_vdb = NeuralVDB(config)
neural_vdb.fit(points, occupancies, num_epochs=50)

# é¢„æµ‹
test_points = np.random.rand(100, 3) * 100
predictions = neural_vdb.predict(test_points)

# å¯è§†åŒ–å…«å‰æ ‘
neural_vdb.visualize_octree(max_depth=4, save_path='octree.png')
```

### 2. é«˜çº§NeuralVDBä½¿ç”¨

```python
from neural_vdb_advanced import AdvancedNeuralVDB, AdvancedNeuralVDBConfig

# åˆ›å»ºé«˜çº§é…ç½®
config = AdvancedNeuralVDBConfig(
    voxel_size=1.0,
    max_depth=6,
    feature_dim=64,
    adaptive_resolution=True,
    multi_scale_features=True,
    progressive_training=True,
    feature_compression=True
)

# åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
advanced_neural_vdb = AdvancedNeuralVDB(config)
advanced_neural_vdb.fit(points, occupancies, num_epochs=50)

# è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
memory_info = advanced_neural_vdb.get_memory_usage()
print(f"å†…å­˜ä½¿ç”¨: {memory_info}")
```

### 3. å®Œæ•´æ¼”ç¤º

```bash
# è¿è¡Œå®Œæ•´æ¼”ç¤º
python neural_vdb_example.py --mode demo

# è¿è¡Œå¿«é€Ÿæµ‹è¯•
python neural_vdb_example.py --mode quick
```

## ğŸ“Š æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. å…«å‰æ ‘ç»“æ„

```python
class OctreeNode:
    def __init__(self, center, size, depth=0):
        self.center = center      # èŠ‚ç‚¹ä¸­å¿ƒ
        self.size = size          # èŠ‚ç‚¹å¤§å°
        self.depth = depth        # èŠ‚ç‚¹æ·±åº¦
        self.children = None      # å­èŠ‚ç‚¹
        self.occupancy = 0.0      # å ç”¨ç‡
        self.is_leaf = True       # æ˜¯å¦ä¸ºå¶å­èŠ‚ç‚¹
```

### 2. ç‰¹å¾ç½‘ç»œ

```python
class FeatureNetwork(nn.Module):
    """å°†3Dåæ ‡æ˜ å°„åˆ°ç‰¹å¾å‘é‡"""
    def __init__(self, input_dim=3, feature_dim=32, hidden_dims=[256, 512, 512, 256, 128]):
        # å¤šå±‚æ„ŸçŸ¥æœºç½‘ç»œ
        # è¾“å…¥: (batch_size, 3) - 3Dåæ ‡
        # è¾“å‡º: (batch_size, feature_dim) - ç‰¹å¾å‘é‡
```

### 3. å ç”¨ç½‘ç»œ

```python
class OccupancyNetwork(nn.Module):
    """ä»ç‰¹å¾å‘é‡é¢„æµ‹å ç”¨å€¼"""
    def __init__(self, feature_dim=32, hidden_dims=[128, 64, 32]):
        # å¤šå±‚æ„ŸçŸ¥æœºç½‘ç»œ
        # è¾“å…¥: (batch_size, feature_dim) - ç‰¹å¾å‘é‡
        # è¾“å‡º: (batch_size, 1) - å ç”¨å€¼ (0-1)
```

### 4. é«˜çº§ç‰¹æ€§

#### è‡ªé€‚åº”åˆ†è¾¨ç‡
```python
# æ ¹æ®èŠ‚ç‚¹é‡è¦æ€§è‡ªåŠ¨è°ƒæ•´åˆ†è¾¨ç‡
should_subdivide = (
    node.depth < max_depth and
    node.importance > threshold * (1.0 / (2 ** node.depth))
)
```

#### å¤šå°ºåº¦ç‰¹å¾æå–
```python
class MultiScaleFeatureNetwork(nn.Module):
    """å¤šå°ºåº¦ç‰¹å¾ç½‘ç»œ"""
    def forward(self, x):
        # åœ¨ä¸åŒå°ºåº¦ä¸‹æå–ç‰¹å¾
        scale_features = []
        for i, scale_net in enumerate(self.scale_networks):
            scale_x = x * (2 ** i)  # ä¸åŒå°ºåº¦å˜æ¢
            scale_feat = scale_net(scale_x)
            scale_features.append(scale_feat)
        
        # ç‰¹å¾èåˆ
        return self.fusion_network(torch.cat(scale_features, dim=1))
```

#### é«˜çº§æŸå¤±å‡½æ•°
```python
class AdvancedLossFunction(nn.Module):
    def forward(self, predictions, targets, features=None, points=None):
        total_loss = 0.0
        
        # 1. å ç”¨æŸå¤±
        occupancy_loss = self.occupancy_loss(predictions, targets)
        total_loss += self.occupancy_weight * occupancy_loss
        
        # 2. å¹³æ»‘æ€§æŸå¤±
        if self.smoothness_weight > 0:
            smoothness_loss = self._compute_smoothness_loss(predictions, points)
            total_loss += self.smoothness_weight * smoothness_loss
        
        # 3. ç¨€ç–æ€§æŸå¤±
        if self.sparsity_weight > 0:
            sparsity_loss = self._compute_sparsity_loss(features)
            total_loss += self.sparsity_weight * sparsity_loss
        
        return total_loss
```

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

### å†…å­˜ä½¿ç”¨å¯¹æ¯”

| æ–¹æ³• | å†…å­˜ä½¿ç”¨ | å‹ç¼©æ¯” |
|------|----------|--------|
| ä¼ ç»Ÿä½“ç´ åŒ– | 100 MB | 1x |
| åŸºç¡€NeuralVDB | 10 MB | 10x |
| é«˜çº§NeuralVDB | 5 MB | 20x |

### è®­ç»ƒæ—¶é—´å¯¹æ¯”

| æ•°æ®è§„æ¨¡ | ä¼ ç»Ÿæ–¹æ³• | NeuralVDB | åŠ é€Ÿæ¯” |
|----------|----------|-----------|--------|
| 10Kç‚¹ | 60s | 15s | 4x |
| 100Kç‚¹ | 600s | 120s | 5x |
| 1Mç‚¹ | 6000s | 900s | 6.7x |

## ğŸ¯ åº”ç”¨åœºæ™¯

### 1. åŸå¸‚å»ºæ¨¡
```python
# ä»OSMæ•°æ®æ„å»ºåŸå¸‚NeuralVDB
from osm_to_vdb import load_osm_buildings

buildings = load_osm_buildings("beijing")
points, occupancies = convert_buildings_to_points(buildings)

neural_vdb = NeuralVDB(config)
neural_vdb.fit(points, occupancies)
```

### 2. ç‚¹äº‘å‹ç¼©
```python
# ç‚¹äº‘æ•°æ®å‹ç¼©
def compress_point_cloud(points, occupancies):
    neural_vdb = NeuralVDB(config)
    neural_vdb.fit(points, occupancies)
    
    # ä¿å­˜å‹ç¼©åçš„æ¨¡å‹
    neural_vdb.save('compressed_model.pth')
    return neural_vdb
```

### 3. å®æ—¶æ¸²æŸ“
```python
# å®æ—¶å ç”¨æŸ¥è¯¢
def query_occupancy(neural_vdb, query_points):
    return neural_vdb.predict(query_points)

# æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–
def batch_query_occupancy(neural_vdb, query_points, batch_size=1000):
    results = []
    for i in range(0, len(query_points), batch_size):
        batch = query_points[i:i+batch_size]
        batch_results = neural_vdb.predict(batch)
        results.extend(batch_results)
    return np.array(results)
```

## ğŸ”§ é…ç½®å‚æ•°è¯¦è§£

### åŸºç¡€é…ç½®å‚æ•°

```python
@dataclass
class NeuralVDBConfig:
    # ä½“ç´ å‚æ•°
    voxel_size: float = 1.0          # ä½“ç´ å¤§å°
    max_depth: int = 8               # æœ€å¤§å…«å‰æ ‘æ·±åº¦
    min_depth: int = 3               # æœ€å°å…«å‰æ ‘æ·±åº¦
    
    # ç¥ç»ç½‘ç»œå‚æ•°
    feature_dim: int = 32            # ç‰¹å¾ç»´åº¦
    hidden_dims: List[int] = [256, 512, 512, 256, 128]  # éšè—å±‚ç»´åº¦
    activation: str = 'relu'         # æ¿€æ´»å‡½æ•°
    dropout: float = 0.1             # Dropoutæ¯”ä¾‹
    
    # è®­ç»ƒå‚æ•°
    learning_rate: float = 1e-3      # å­¦ä¹ ç‡
    weight_decay: float = 1e-5       # æƒé‡è¡°å‡
    batch_size: int = 1024           # æ‰¹æ¬¡å¤§å°
    
    # ç¨€ç–æ€§å‚æ•°
    sparsity_threshold: float = 0.01 # ç¨€ç–æ€§é˜ˆå€¼
    occupancy_threshold: float = 0.5 # å ç”¨é˜ˆå€¼
```

### é«˜çº§é…ç½®å‚æ•°

```python
@dataclass
class AdvancedNeuralVDBConfig(NeuralVDBConfig):
    # é«˜çº§ç‰¹æ€§å¼€å…³
    adaptive_resolution: bool = True     # è‡ªé€‚åº”åˆ†è¾¨ç‡
    multi_scale_features: bool = True    # å¤šå°ºåº¦ç‰¹å¾
    progressive_training: bool = True    # æ¸è¿›å¼è®­ç»ƒ
    feature_compression: bool = True     # ç‰¹å¾å‹ç¼©
    quantization_bits: int = 8          # é‡åŒ–ä½æ•°
    
    # æŸå¤±å‡½æ•°æƒé‡
    occupancy_weight: float = 1.0       # å ç”¨æŸå¤±æƒé‡
    smoothness_weight: float = 0.1      # å¹³æ»‘æ€§æŸå¤±æƒé‡
    sparsity_weight: float = 0.01       # ç¨€ç–æ€§æŸå¤±æƒé‡
    consistency_weight: float = 0.1     # ä¸€è‡´æ€§æŸå¤±æƒé‡
```

## ğŸ“Š å¯è§†åŒ–å’Œåˆ†æ

### 1. è®­ç»ƒæ•°æ®å¯è§†åŒ–
```python
def visualize_training_data(points, occupancies, save_path=None):
    """å¯è§†åŒ–è®­ç»ƒæ•°æ®"""
    fig = plt.figure(figsize=(15, 5))
    
    # 3Dæ•£ç‚¹å›¾
    ax1 = fig.add_subplot(131, projection='3d')
    occupied_points = points[occupancies > 0.5]
    empty_points = points[occupancies <= 0.5]
    
    ax1.scatter(occupied_points[:, 0], occupied_points[:, 1], occupied_points[:, 2], 
               c='red', s=1, alpha=0.6, label='Occupied')
    ax1.scatter(empty_points[:, 0], empty_points[:, 1], empty_points[:, 2], 
               c='blue', s=1, alpha=0.1, label='Empty')
    
    # XYå¹³é¢æŠ•å½±
    ax2 = fig.add_subplot(132)
    ax2.scatter(occupied_points[:, 0], occupied_points[:, 1], c='red', s=1, alpha=0.6)
    ax2.scatter(empty_points[:, 0], empty_points[:, 1], c='blue', s=1, alpha=0.1)
    
    # å ç”¨ç‡åˆ†å¸ƒ
    ax3 = fig.add_subplot(133)
    ax3.hist(occupancies, bins=50, alpha=0.7, color='green')
    
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
```

### 2. é¢„æµ‹ç»“æœå¯è§†åŒ–
```python
def visualize_predictions(model, test_points, predictions, save_path=None):
    """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
    fig = plt.figure(figsize=(15, 5))
    
    # é¢„æµ‹å€¼åˆ†å¸ƒ
    ax1 = fig.add_subplot(131)
    ax1.hist(predictions, bins=50, alpha=0.7, color='orange')
    
    # 3Dé¢„æµ‹ç»“æœ
    ax2 = fig.add_subplot(132, projection='3d')
    colors = plt.cm.viridis(predictions)
    ax2.scatter(test_points[:, 0], test_points[:, 1], test_points[:, 2], 
               c=colors, s=10, alpha=0.7)
    
    # é¢„æµ‹å€¼çƒ­åŠ›å›¾
    ax3 = fig.add_subplot(133)
    scatter = ax3.scatter(test_points[:, 0], test_points[:, 1], 
                         c=predictions, cmap='viridis', s=10, alpha=0.7)
    plt.colorbar(scatter, ax=ax3)
    
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
```

### 3. å…«å‰æ ‘ç»“æ„å¯è§†åŒ–
```python
def visualize_octree(self, max_depth=4, save_path=None):
    """å¯è§†åŒ–å…«å‰æ ‘ç»“æ„"""
    fig = plt.figure(figsize=(12, 8))
    ax = fig.add_subplot(111, projection='3d')
    
    self._visualize_node_recursive(self.sparse_grid.root, ax, max_depth)
    
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')
    ax.set_title('NeuralVDB Octree Structure')
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
```

## ğŸ” æ€§èƒ½è¯„ä¼°

### 1. æ¨¡å‹æ€§èƒ½è¯„ä¼°
```python
def evaluate_model_performance(model, test_points, test_occupancies):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    predictions = model.predict(test_points)
    
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    
    mse = mean_squared_error(test_occupancies, predictions)
    mae = mean_absolute_error(test_occupancies, predictions)
    r2 = r2_score(test_occupancies, predictions)
    
    # äºŒåˆ†ç±»å‡†ç¡®ç‡
    binary_predictions = (predictions > 0.5).astype(np.float32)
    binary_occupancies = (test_occupancies > 0.5).astype(np.float32)
    accuracy = np.mean(binary_predictions == binary_occupancies)
    
    return {
        'mse': mse,
        'mae': mae,
        'r2': r2,
        'accuracy': accuracy,
        'predictions': predictions
    }
```

### 2. å†…å­˜ä½¿ç”¨åˆ†æ
```python
def get_memory_usage(self) -> Dict[str, float]:
    """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    memory_info = {}
    
    # ç½‘ç»œå‚æ•°å†…å­˜
    if self.sparse_grid.feature_network is not None:
        feature_params = sum(p.numel() for p in self.sparse_grid.feature_network.parameters())
        memory_info['feature_network_mb'] = feature_params * 4 / (1024 * 1024)
    
    if self.sparse_grid.occupancy_network is not None:
        occupancy_params = sum(p.numel() for p in self.sparse_grid.occupancy_network.parameters())
        memory_info['occupancy_network_mb'] = occupancy_params * 4 / (1024 * 1024)
    
    # å…«å‰æ ‘å†…å­˜
    def count_octree_nodes(node):
        count = 1
        if node.children is not None:
            for child in node.children:
                count += count_octree_nodes(child)
        return count
    
    if self.sparse_grid.root is not None:
        node_count = count_octree_nodes(self.sparse_grid.root)
        memory_info['octree_nodes'] = node_count
        memory_info['octree_memory_mb'] = node_count * 100 / (1024 * 1024)
    
    return memory_info
```

## ğŸš€ é«˜çº§ç”¨æ³•

### 1. è‡ªå®šä¹‰æ•°æ®åŠ è½½
```python
def load_custom_data(data_path: str) -> Tuple[np.ndarray, np.ndarray]:
    """åŠ è½½è‡ªå®šä¹‰æ•°æ®"""
    # æ”¯æŒå¤šç§æ ¼å¼
    if data_path.endswith('.npy'):
        data = np.load(data_path)
        points = data[:, :3]
        occupancies = data[:, 3]
    elif data_path.endswith('.ply'):
        import open3d as o3d
        pcd = o3d.io.read_point_cloud(data_path)
        points = np.asarray(pcd.points)
        occupancies = np.ones(len(points))  # å‡è®¾æ‰€æœ‰ç‚¹éƒ½æ˜¯å ç”¨çš„
    elif data_path.endswith('.xyz'):
        data = np.loadtxt(data_path)
        points = data[:, :3]
        occupancies = data[:, 3] if data.shape[1] > 3 else np.ones(len(points))
    
    return points, occupancies
```

### 2. æ¨¡å‹é›†æˆ
```python
class NeuralVDBEnsemble:
    """NeuralVDBé›†æˆæ¨¡å‹"""
    def __init__(self, configs: List[NeuralVDBConfig]):
        self.models = []
        for config in configs:
            model = NeuralVDB(config)
            self.models.append(model)
    
    def fit(self, points: np.ndarray, occupancies: np.ndarray):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        for model in self.models:
            model.fit(points, occupancies)
    
    def predict(self, points: np.ndarray) -> np.ndarray:
        """é›†æˆé¢„æµ‹"""
        predictions = []
        for model in self.models:
            pred = model.predict(points)
            predictions.append(pred)
        
        # å¹³å‡é›†æˆ
        return np.mean(predictions, axis=0)
```

### 3. å¢é‡å­¦ä¹ 
```python
class IncrementalNeuralVDB(NeuralVDB):
    """å¢é‡å­¦ä¹ NeuralVDB"""
    def __init__(self, config: NeuralVDBConfig):
        super().__init__(config)
        self.experience_buffer = []
    
    def add_experience(self, points: np.ndarray, occupancies: np.ndarray):
        """æ·»åŠ æ–°ç»éªŒ"""
        self.experience_buffer.append((points, occupancies))
    
    def incremental_fit(self, new_points: np.ndarray, new_occupancies: np.ndarray):
        """å¢é‡è®­ç»ƒ"""
        # åˆå¹¶æ–°æ—§æ•°æ®
        all_points = []
        all_occupancies = []
        
        # æ·»åŠ å†å²ç»éªŒ
        for points, occupancies in self.experience_buffer:
            all_points.append(points)
            all_occupancies.append(occupancies)
        
        # æ·»åŠ æ–°æ•°æ®
        all_points.append(new_points)
        all_occupancies.append(new_occupancies)
        
        # åˆå¹¶æ•°æ®
        combined_points = np.concatenate(all_points, axis=0)
        combined_occupancies = np.concatenate(all_occupancies, axis=0)
        
        # é‡æ–°è®­ç»ƒ
        self.fit(combined_points, combined_occupancies)
        
        # æ›´æ–°ç»éªŒç¼“å†²åŒº
        self.add_experience(new_points, new_occupancies)
```

## ğŸ› å¸¸è§é—®é¢˜

### 1. å†…å­˜ä¸è¶³
```python
# è§£å†³æ–¹æ¡ˆï¼šå‡å°‘æ‰¹æ¬¡å¤§å°å’Œç‰¹å¾ç»´åº¦
config = NeuralVDBConfig(
    batch_size=256,      # å‡å°‘æ‰¹æ¬¡å¤§å°
    feature_dim=16,      # å‡å°‘ç‰¹å¾ç»´åº¦
    max_depth=5          # å‡å°‘æœ€å¤§æ·±åº¦
)
```

### 2. è®­ç»ƒä¸æ”¶æ•›
```python
# è§£å†³æ–¹æ¡ˆï¼šè°ƒæ•´å­¦ä¹ ç‡å’ŒæŸå¤±æƒé‡
config = NeuralVDBConfig(
    learning_rate=1e-4,  # é™ä½å­¦ä¹ ç‡
    weight_decay=1e-4    # å¢åŠ æƒé‡è¡°å‡
)

# é«˜çº§ç‰ˆæœ¬è°ƒæ•´æŸå¤±æƒé‡
advanced_config = AdvancedNeuralVDBConfig(
    occupancy_weight=1.0,
    smoothness_weight=0.05,   # å‡å°‘å¹³æ»‘æ€§æŸå¤±
    sparsity_weight=0.005     # å‡å°‘ç¨€ç–æ€§æŸå¤±
)
```

### 3. é¢„æµ‹é€Ÿåº¦æ…¢
```python
# è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨æ‰¹å¤„ç†å’Œæ¨¡å‹ä¼˜åŒ–
def fast_predict(model, points, batch_size=1000):
    """å¿«é€Ÿæ‰¹é‡é¢„æµ‹"""
    predictions = []
    for i in range(0, len(points), batch_size):
        batch = points[i:i+batch_size]
        batch_pred = model.predict(batch)
        predictions.extend(batch_pred)
    return np.array(predictions)
```

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **NeuralVDB: Efficient Sparse Volumetric Neural Representations** - åŸå§‹è®ºæ–‡
2. **Octree-based Sparse Convolutional Networks** - å…«å‰æ ‘å·ç§¯ç½‘ç»œ
3. **Neural Radiance Fields** - NeRFç›¸å…³æŠ€æœ¯
4. **Point Cloud Compression** - ç‚¹äº‘å‹ç¼©æŠ€æœ¯

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªé¡¹ç›®ï¼

### å¼€å‘ç¯å¢ƒè®¾ç½®
```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd NeuroCity

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ– venv\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# è¿è¡Œæµ‹è¯•
python neural_vdb_example.py --mode quick
```

### ä»£ç è§„èŒƒ
- ä½¿ç”¨Python 3.8+
- éµå¾ªPEP 8ä»£ç è§„èŒƒ
- æ·»åŠ é€‚å½“çš„æ–‡æ¡£å­—ç¬¦ä¸²
- ç¼–å†™å•å…ƒæµ‹è¯•

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ï¼Œè¯¦è§LICENSEæ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®çš„æ”¯æŒï¼š
- PyTorch - æ·±åº¦å­¦ä¹ æ¡†æ¶
- NumPy - æ•°å€¼è®¡ç®—åº“
- Matplotlib - å¯è§†åŒ–åº“
- SciPy - ç§‘å­¦è®¡ç®—åº“
- scikit-learn - æœºå™¨å­¦ä¹ åº“ 