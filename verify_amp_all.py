#!/usr/bin/env python3
"""
éªŒè¯æ‰€æœ‰nerfsæ¨¡å—çš„AMPä¼˜åŒ–
"""

import warnings
import torch
import sys
import os

def test_amp_imports():
    """æµ‹è¯•AMPå¯¼å…¥æ˜¯å¦æ­£å¸¸"""
    print("ğŸ” æµ‹è¯•AMPå¯¼å…¥...")
    
    try:
        from torch.amp.autocast_mode import autocast
        from torch.amp.grad_scaler import GradScaler
        print("âœ… AMPå¯¼å…¥æˆåŠŸ")
        return True
    except ImportError as e:
        print(f"âŒ AMPå¯¼å…¥å¤±è´¥: {e}")
        return False

def test_amp_functionality():
    """æµ‹è¯•AMPåŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print("ğŸ” æµ‹è¯•AMPåŠŸèƒ½...")
    
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter("always")
        
        try:
            from torch.amp.autocast_mode import autocast
            from torch.amp.grad_scaler import GradScaler
            
            # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
            scaler = GradScaler()
            device_type = "cuda" if torch.cuda.is_available() else "cpu"
            
            with autocast(device_type=device_type):
                x = torch.randn(10, 10, requires_grad=True)
                y = x * 2
                loss = y.sum()
            
            scaled_loss = scaler.scale(loss)
            scaled_loss.backward()
            
            # æ£€æŸ¥è¿‡æ—¶è­¦å‘Š
            deprecation_warnings = [
                warning for warning in w 
                if issubclass(warning.category, (DeprecationWarning, FutureWarning))
            ]
            
            if deprecation_warnings:
                print(f"âŒ å‘ç° {len(deprecation_warnings)} ä¸ªè¿‡æ—¶è­¦å‘Š:")
                for warning in deprecation_warnings:
                    print(f"  - {warning.message}")
                return False
            else:
                print("âœ… AMPåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼Œæ²¡æœ‰è¿‡æ—¶è­¦å‘Š")
                return True
                
        except Exception as e:
            print(f"âŒ AMPåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            return False

def check_nerfs_modules():
    """æ£€æŸ¥æ‰€æœ‰nerfsæ¨¡å—çš„å¯¼å…¥"""
    print("ğŸ” æ£€æŸ¥nerfsæ¨¡å—å¯¼å…¥...")
    
    # æ·»åŠ srcåˆ°è·¯å¾„
    sys.path.insert(0, os.path.abspath('src'))
    
    modules_to_test = [
        'nerfs.svraster.core',
        'nerfs.plenoxels.core', 
        # å¯ä»¥æ·»åŠ æ›´å¤šæ¨¡å—æµ‹è¯•
    ]
    
    success_count = 0
    
    for module_name in modules_to_test:
        try:
            # åˆ›å»ºä¸´æ—¶æ¨¡å—é¿å…ç›¸å¯¹å¯¼å…¥é—®é¢˜
            import importlib.util
            
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            file_path = f"src/{module_name.replace('.', '/')}.py"
            
            if os.path.exists(file_path):
                with open(file_path, 'r') as f:
                    content = f.read()
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡æ—¶çš„AMP API
                if 'torch.cuda.amp' in content:
                    print(f"âŒ {module_name}: ä»ç„¶ä½¿ç”¨è¿‡æ—¶çš„torch.cuda.amp")
                elif 'torch.amp.autocast(' in content or 'torch.amp.GradScaler(' in content:
                    print(f"âŒ {module_name}: ä»ç„¶ä½¿ç”¨é”™è¯¯çš„torch.ampç›´æ¥è°ƒç”¨")
                else:
                    print(f"âœ… {module_name}: AMPä½¿ç”¨æ­£ç¡®")
                    success_count += 1
            else:
                print(f"âš ï¸ {module_name}: æ–‡ä»¶ä¸å­˜åœ¨")
                
        except Exception as e:
            print(f"âŒ {module_name}: æ£€æŸ¥å¤±è´¥ - {e}")
    
    return success_count, len(modules_to_test)

def main():
    print("ğŸš€ å¼€å§‹éªŒè¯æ‰€æœ‰nerfsæ¨¡å—çš„AMPä¼˜åŒ–...\n")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}\n")
    
    all_passed = True
    
    # æµ‹è¯•AMPå¯¼å…¥
    if not test_amp_imports():
        all_passed = False
    
    print()
    
    # æµ‹è¯•AMPåŠŸèƒ½
    if not test_amp_functionality():
        all_passed = False
    
    print()
    
    # æ£€æŸ¥nerfsæ¨¡å—
    success, total = check_nerfs_modules()
    
    print(f"\nğŸ“Š æ£€æŸ¥ç»“æœ:")
    print(f"âœ… æˆåŠŸ: {success}/{total} ä¸ªæ¨¡å—")
    
    if success < total:
        all_passed = False
    
    print("\n" + "="*60)
    
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰AMPä¼˜åŒ–éªŒè¯é€šè¿‡ï¼")
        print("âœ… æ‰€æœ‰è¿‡æ—¶çš„torch.cuda.amp APIå·²æ›´æ–°")
        print("âœ… ä½¿ç”¨ç°ä»£çš„torch.amp API")
        print("âœ… æ²¡æœ‰è¿‡æ—¶å‡½æ•°è­¦å‘Š")
        print("âœ… å…¼å®¹PyTorch 2.x+")
    else:
        print("âŒ éƒ¨åˆ†éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯")
    
    return all_passed

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
